{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of topics for the final project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 algorithmic fairness\n",
    "Algorithm fairness is becoming a fundamental topic in ML. It is a complex ethical task to define what fairness is/means. Once we have defined quantitatively what fairness is then, from a mathematical perspective, the problem of  algorithmic fairness is very clear: it is a multi-objective optimization problem. There are multi objectives that we aim to optimize during data fitting, e.g., accuracy and fairness.\n",
    "\n",
    "The goal of this project is to implement from scratch the \"fair\" **linear and nonlinear SVM** described in the following paper (see in particular Appendix A that reports the optimization problem)  \n",
    "\n",
    "[\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf)\n",
    "\n",
    "and reproduce the experiments reported in the paper. In particular, apply your method to the Adult and Bank \n",
    "data sets.\n",
    "\n",
    "Your notebook must include:\n",
    "* a description (summary) of the algorithm presented in the above paper (focusing on SVM), similar to the theoretical details of logistic regression I wrote at the beginning of the notebook for e-tivity Task A (week 1&2). The reader must understand from your explanation the difference between standard SVM and the \"fair\" SVM.\n",
    "* You implementation of the \"fair\" **linear and nonlinear SVM** using CVXOPT to solve data fitting (as I have shown in Week 3 webinar, see also example below). You should implement it as a Python class (similar to logistic regression class for E-tivity 1).\n",
    "* A test of the input-output behavior of your algorithm. More clearly, you have to replicate the experiment results you find in Section 4.1 for Synthetic Data and Section 4.2 of the above paper for the Adult and Bank data sets.\n",
    "\n",
    "\n",
    "Resources:\n",
    "* Week 3 webinar slides with the details of the SVM algorithm;\n",
    "* [Example](https://xavierbourretsicotte.github.io/SVM_implementation.html) about how to use the library CVXOPT to implement data fitting for standard SVM\n",
    "* [fairness-in-machine-learning](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)\n",
    "* (Optional) Multi-objective optimization and Pareto optimality see Book chapter 12 (of our Module's book).\n",
    "\n",
    "**How to approach the problem** (this is just a suggestion).\n",
    "\n",
    "You can start implementing linear SVM and apply it to the Synthetic Data experiment in Section 4.1 so that you can plot the classification line for standard linear SVM versus fair linear SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:45.225887Z",
     "start_time": "2020-02-15T12:53:29.993350Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Import helper functions\n",
    "from mlfromscratch.utils import train_test_split, normalize, accuracy_score, Plot\n",
    "#from mlfromscratch.utils.kernels import *\n",
    "from mlfromscratch.supervised_learning import SupportVectorMachine\n",
    "\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:48.097038Z",
     "start_time": "2020-02-15T12:53:47.888157Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "from cvxopt import matrix\n",
    "from mlfromscratch.utils import train_test_split, normalize, accuracy_score\n",
    "from mlfromscratch.utils.kernels import *\n",
    "from mlfromscratch.utils import Plot\n",
    "\n",
    "# Hide cvxopt output\n",
    "cvxopt.solvers.options['show_progress'] = True\n",
    "\n",
    "class SupportVectorMachine(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=None, kernel=rbf_kernel, sensible_feature=None, power=4, gamma=None, coef=4):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        if self.fairness:\n",
    "            self.values_of_sensible_feature = list(set(self.sensible_feature))\n",
    "            self.list_of_sensible_feature_train = self.sensible_feature\n",
    "            self.val0 = np.min(self.values_of_sensible_feature)\n",
    "            self.val1 = np.max(self.values_of_sensible_feature)\n",
    "            self.set_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                           and self.sensible_feature[idx] == self.val1]\n",
    "            self.set_not_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                               and self.sensible_feature[idx] == self.val0]\n",
    "            self.set_1 = [idx for idx, ex in enumerate(X) if y[idx] == 1]\n",
    "            self.n_A1 = len(self.set_A1)\n",
    "            self.n_not_A1 = len(self.set_not_A1)\n",
    "            self.n_1 = len(self.set_1)  \n",
    "            \n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if not self.C:\n",
    "            G = cvxopt.matrix(np.identity(n_samples) * -1)\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            G_max = np.identity(n_samples) * -1\n",
    "            G_min = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n",
    "            h_max = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n",
    "            \n",
    "        # Stack the fairness constraint\n",
    "        if self.fairness:\n",
    "            tau = [(np.sum(kernel_matrix[self.set_A1, idx]) / self.n_A1) - (np.sum(kernel_matrix[self.set_not_A1, idx]) / self.n_not_A1) for idx in range(len(y))]\n",
    "            print(f\"Tauji kee value {tau[0:10]}\")\n",
    "            fairness_line = matrix(y * tau, (1, n_samples), 'd')\n",
    "            print(\"Fairness_line\", fairness_line)\n",
    "            A = cvxopt.matrix(np.vstack([A, fairness_line]))\n",
    "            b = cvxopt.matrix([0.0, 0.0])            \n",
    "            \n",
    "        print(\"P.shape\", P.size)\n",
    "        print(\"q.shape\", q.size)\n",
    "        print(\"G.shape\", G.size)\n",
    "        print(\"h.shape\", h.size)\n",
    "        print(\"A.shape\", A.size)\n",
    "        print(\"b.shape\", b.size)\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "\n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > 1e-7\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0, './fair-classification3/fair_classification') # the code for fair classification is in this directory\n",
    "\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import numpy as np\n",
    "from random import seed, shuffle\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "SEED = 1122\n",
    "seed(SEED) # set the random seed so that the random permutations can be reproduced again\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The adult dataset can be obtained from: http://archive.ics.uci.edu/ml/datasets/Adult\n",
    "    The code will look for the data files (adult.data, adult.test) in the present directory, if they are not found, it will download them from UCI archive.\n",
    "\"\"\"\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\") # get the current directory listing\n",
    "    print(\"Looking for file '%s' in the current directory...\" % fname)\n",
    "\n",
    "    if fname not in files:\n",
    "        print(\"'%s' not found! Downloading from UCI Archive...\" % fname)\n",
    "        addr = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/%s\" % fname\n",
    "        response = urllib.request.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"w\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print(\"'%s' download and saved locally..\" % fname)\n",
    "    else:\n",
    "        print(\"File found in current directory..\")\n",
    "    \n",
    "    print()\n",
    "    return\n",
    "\n",
    "        \n",
    "def load_adult_data(load_data_size=None, drop_sensitive=True):\n",
    "\n",
    "   # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    attrs = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country','target'] \n",
    "    \n",
    "    data = pd.read_csv(\"dataset/adult/adult.data\", names=attrs, skipinitialspace=True) \n",
    "\n",
    "    data = data.replace(to_replace ='?', value =np.NaN)\n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    target_mapping = {'<=50K': -1, '>50K': 1,'<=50K.': -1, '>50K.': 1}\n",
    "    data.replace({\"target\": target_mapping}, inplace=True)\n",
    "    data['native_country'] = data['native_country'].apply(lambda x: 'USA' if x in ['United-States'] else 'NON-USA')\n",
    "    data['education'] = data['education'].apply(lambda x: 'pre-middle-school' if x in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"] else 'high-school')\n",
    "\n",
    "    encoded_object_df = pd.DataFrame()\n",
    "\n",
    "    for column in ['workclass', 'sex', 'education', 'marital_status', 'occupation','relationship',  'native_country']:\n",
    "        # race\n",
    "        encoded_object_df = pd.concat([encoded_object_df,pd.get_dummies(data[column], prefix=column, drop_first=True)] ,axis=1)\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    cols_to_scale = ['age', 'education_num', 'capital_gain', 'capital_loss', 'capital_loss', 'target']\n",
    "    #fnlwgt\n",
    "\n",
    "    encoded_int_df = data[cols_to_scale]\n",
    "\n",
    "    encoded_int_df[cols_to_scale] = min_max_scaler.fit_transform(encoded_int_df[cols_to_scale])\n",
    "    \n",
    "    final_df = pd.concat([encoded_object_df, encoded_int_df], axis=1)\n",
    "    #final_df = final_df.sample(n=load_data_size)\n",
    "    final_df = final_df.iloc[0:load_data_size]\n",
    "    \n",
    "    # shuffle the data\n",
    "    \n",
    "    y = np.where(final_df['target'] == 0, -1, 1)\n",
    "    x_sensitive = np.array(final_df['sex_Male'])\n",
    "    \n",
    "    if drop_sensitive:\n",
    "        final_df.drop(['sex_Male'], axis=1, inplace=True)\n",
    "        \n",
    "    final_df.drop(['target'], axis=1, inplace=True)\n",
    "        \n",
    "    X = np.array(final_df)\n",
    "    \n",
    "    print(f\"Shapes of X = {X.shape}, y={y.shape}, x_sensitive={x_sensitive.shape}\")\n",
    "    \n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of X = (2000, 37), y=(2000,), x_sensitive=(2000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vipulpopat/Tools/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/vipulpopat/Tools/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "data_count=2000\n",
    "X, y, x_control = load_adult_data(load_data_size=data_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 37)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_control.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel_cust(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def linear_kernel(**kwargs):\n",
    "    def f(x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "    return f\n",
    "\n",
    "\n",
    "def polynomial_kernel(power, coef, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        return (np.dot(x1, x2) + coef)**power\n",
    "    return f\n",
    "\n",
    "\n",
    "def rbf_kernel(gamma, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        distance = np.linalg.norm(x1 - x2) ** 2\n",
    "        return np.exp(-gamma * distance)\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1,None\n",
      "P.shape (2000, 2000)\n",
      "q.shape (2000, 1)\n",
      "G.shape (4000, 2000)\n",
      "h.shape (4000, 1)\n",
      "A.shape (1, 2000)\n",
      "b.shape (1, 1)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.8266e+02 -5.3304e+03  3e+04  3e+00  2e-14\n",
      " 1: -6.6197e+02 -3.5473e+03  4e+03  2e-01  2e-14\n",
      " 2: -7.0830e+02 -1.1924e+03  5e+02  2e-02  1e-14\n",
      " 3: -7.9469e+02 -9.7105e+02  2e+02  6e-03  1e-14\n",
      " 4: -8.2385e+02 -9.1260e+02  9e+01  2e-03  1e-14\n",
      " 5: -8.3924e+02 -8.8427e+02  5e+01  9e-04  1e-14\n",
      " 6: -8.4789e+02 -8.6940e+02  2e+01  4e-04  1e-14\n",
      " 7: -8.5286e+02 -8.6124e+02  8e+00  9e-05  2e-14\n",
      " 8: -8.5497e+02 -8.5821e+02  3e+00  2e-05  2e-14\n",
      " 9: -8.5603e+02 -8.5687e+02  8e-01  3e-06  2e-14\n",
      "10: -8.5633e+02 -8.5652e+02  2e-01  7e-07  2e-14\n",
      "11: -8.5641e+02 -8.5643e+02  2e-02  7e-08  2e-14\n",
      "12: -8.5642e+02 -8.5642e+02  8e-04  3e-10  2e-14\n",
      "Optimal solution found.\n",
      "Accuracy: 0.7955\n",
      "=====================================================\n",
      "C=1,With Feature\n",
      "Tauji kee value [0.0038795206756954137, 0.02923418760620511, 0.012224612795177614, 0.02854025020556661, -0.014353352396609842, -0.0024951060375071332, 0.00850797669938308, 0.029359780295757365, 0.0012680425066285972, 0.03219567725322936]\n",
      "Fairness_line [-3.88e-03 -2.92e-02 -1.22e-02 -2.85e-02  1.44e-02  2.50e-03 -8.51e-03 ... ]\n",
      "\n",
      "P.shape (2000, 2000)\n",
      "q.shape (2000, 1)\n",
      "G.shape (4000, 2000)\n",
      "h.shape (4000, 1)\n",
      "A.shape (2, 2000)\n",
      "b.shape (2, 1)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.8279e+02 -5.3311e+03  3e+04  3e+00  2e-14\n",
      " 1: -6.6277e+02 -3.5473e+03  4e+03  2e-01  2e-14\n",
      " 2: -7.0839e+02 -1.1970e+03  5e+02  2e-02  1e-14\n",
      " 3: -7.9436e+02 -9.7165e+02  2e+02  6e-03  1e-14\n",
      " 4: -8.2288e+02 -9.1359e+02  9e+01  2e-03  1e-14\n",
      " 5: -8.3868e+02 -8.8417e+02  5e+01  9e-04  1e-14\n",
      " 6: -8.4713e+02 -8.6958e+02  2e+01  4e-04  1e-14\n",
      " 7: -8.5227e+02 -8.6117e+02  9e+00  1e-04  2e-14\n",
      " 8: -8.5448e+02 -8.5791e+02  3e+00  3e-05  2e-14\n",
      " 9: -8.5561e+02 -8.5642e+02  8e-01  4e-06  2e-14\n",
      "10: -8.5591e+02 -8.5607e+02  2e-01  6e-07  2e-14\n",
      "11: -8.5598e+02 -8.5599e+02  2e-02  3e-08  2e-14\n",
      "12: -8.5598e+02 -8.5598e+02  3e-04  6e-10  2e-14\n",
      "Optimal solution found.\n",
      "Accuracy: 0.744\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "#X_control_train, X_control_test, y_train, y_test = train_test_split(x_control, y, test_size=0.33)\n",
    "\n",
    "C_1=1\n",
    "for C in [C_1]:\n",
    "    \n",
    "    for feature in [None, x_control]:\n",
    "        p_feature = None\n",
    "        if (feature is not None):\n",
    "            p_feature = 'With Feature'\n",
    "        print(f'C={C},{p_feature}')\n",
    "        clf = SupportVectorMachine(kernel=rbf_kernel, sensible_feature=feature, C=C, power=4, coef=1)\n",
    "        #clf = SupportVectorMachine(kernel=polynomial_kernel, sensible_feature=x_control)\n",
    "        #clf.fit(X_train, y_train)\n",
    "        clf.fit(X, y)\n",
    "        #y_pred = clf.predict(X_test)\n",
    "        y_pred = clf.predict(X)\n",
    "\n",
    "        #accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print (\"Accuracy:\", accuracy)\n",
    "        print(\"=====================================================\")\n",
    "        # Reduce dimension to two using PCA and plot the results\n",
    "        #Plot().plot_in_2d(X, y_pred, title=\"Support Vector Machine\", accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "Accuracy of SVM Classifier: 0.8315\n"
     ]
    }
   ],
   "source": [
    "#X_svm, y_svm, x_control_svm = load_adult_data(load_data_size=data_count, drop_sensitive=False)\n",
    "X_svm, y_svm, x_control_svm = X, y, x_control\n",
    "print(y.shape)\n",
    "\n",
    "C = C_1 # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='rbf', C=1).fit(X_svm, y_svm)\n",
    "\n",
    "y_pred_svm = svc.predict(X_svm)\n",
    "accuracy_svm = accuracy_score(y_svm, y_pred_svm)\n",
    "print (\"Accuracy of SVM Classifier:\", accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 37)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of topics for the final project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 algorithmic fairness\n",
    "Algorithm fairness is becoming a fundamental topic in ML. It is a complex ethical task to define what fairness is/means. Once we have defined quantitatively what fairness is then, from a mathematical perspective, the problem of  algorithmic fairness is very clear: it is a multi-objective optimization problem. There are multi objectives that we aim to optimize during data fitting, e.g., accuracy and fairness.\n",
    "\n",
    "The goal of this project is to implement from scratch the \"fair\" **linear and nonlinear SVM** described in the following paper (see in particular Appendix A that reports the optimization problem)  \n",
    "\n",
    "[\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf)\n",
    "\n",
    "and reproduce the experiments reported in the paper. In particular, apply your method to the Adult and Bank \n",
    "data sets.\n",
    "\n",
    "Your notebook must include:\n",
    "* a description (summary) of the algorithm presented in the above paper (focusing on SVM), similar to the theoretical details of logistic regression I wrote at the beginning of the notebook for e-tivity Task A (week 1&2). The reader must understand from your explanation the difference between standard SVM and the \"fair\" SVM.\n",
    "* You implementation of the \"fair\" **linear and nonlinear SVM** using CVXOPT to solve data fitting (as I have shown in Week 3 webinar, see also example below). You should implement it as a Python class (similar to logistic regression class for E-tivity 1).\n",
    "* A test of the input-output behavior of your algorithm. More clearly, you have to replicate the experiment results you find in Section 4.1 for Synthetic Data and Section 4.2 of the above paper for the Adult and Bank data sets.\n",
    "\n",
    "\n",
    "Resources:\n",
    "* Week 3 webinar slides with the details of the SVM algorithm;\n",
    "* [Example](https://xavierbourretsicotte.github.io/SVM_implementation.html) about how to use the library CVXOPT to implement data fitting for standard SVM\n",
    "* [fairness-in-machine-learning](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)\n",
    "* (Optional) Multi-objective optimization and Pareto optimality see Book chapter 12 (of our Module's book).\n",
    "\n",
    "**How to approach the problem** (this is just a suggestion).\n",
    "\n",
    "You can start implementing linear SVM and apply it to the Synthetic Data experiment in Section 4.1 so that you can plot the classification line for standard linear SVM versus fair linear SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:45.225887Z",
     "start_time": "2020-02-15T12:53:29.993350Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import helper functions\n",
    "from mlfromscratch.utils import train_test_split, normalize, accuracy_score, Plot\n",
    "#from mlfromscratch.utils.kernels import *\n",
    "from mlfromscratch.supervised_learning import SupportVectorMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:48.097038Z",
     "start_time": "2020-02-15T12:53:47.888157Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "from cvxopt import matrix\n",
    "from mlfromscratch.utils import train_test_split, normalize, accuracy_score\n",
    "from mlfromscratch.utils.kernels import *\n",
    "from mlfromscratch.utils import Plot\n",
    "\n",
    "# Hide cvxopt output\n",
    "cvxopt.solvers.options['show_progress'] = True\n",
    "\n",
    "class SupportVectorMachine(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=None, kernel=rbf_kernel, sensible_feature=None, power=4, gamma=None, coef=4):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        if self.fairness:\n",
    "            self.values_of_sensible_feature = list(set(self.sensible_feature))\n",
    "            self.list_of_sensible_feature_train = self.sensible_feature\n",
    "            self.val0 = np.min(self.values_of_sensible_feature)\n",
    "            self.val1 = np.max(self.values_of_sensible_feature)\n",
    "            self.set_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                           and self.sensible_feature[idx] == self.val1]\n",
    "            self.set_not_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                               and self.sensible_feature[idx] == self.val0]\n",
    "            self.set_1 = [idx for idx, ex in enumerate(X) if y[idx] == 1]\n",
    "            self.n_A1 = len(self.set_A1)\n",
    "            self.n_not_A1 = len(self.set_not_A1)\n",
    "            self.n_1 = len(self.set_1)  \n",
    "            \n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if not self.C:\n",
    "            G = cvxopt.matrix(np.identity(n_samples) * -1)\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            G_max = np.identity(n_samples) * -1\n",
    "            G_min = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n",
    "            h_max = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n",
    "            \n",
    "        # Stack the fairness constraint\n",
    "        if self.fairness:\n",
    "            tau = [(np.sum(kernel_matrix[self.set_A1, idx]) / self.n_A1) - (np.sum(kernel_matrix[self.set_not_A1, idx]) / self.n_not_A1) for idx in range(len(y))]\n",
    "            fairness_line = matrix(y * tau, (1, n_samples), 'd')\n",
    "            A = cvxopt.matrix(np.vstack([A, fairness_line]))\n",
    "            b = cvxopt.matrix([0.0, 0.0])            \n",
    "            \n",
    "        print(\"P.shape\", P.size)\n",
    "        print(\"q.shape\", q.size)\n",
    "        print(\"G.shape\", G.size)\n",
    "        print(\"h.shape\", h.size)\n",
    "        print(\"A.shape\", A.size)\n",
    "        print(\"b.shape\", b.size)\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "\n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > 1e-7\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0, './fair-classification3/fair_classification') # the code for fair classification is in this directory\n",
    "\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import numpy as np\n",
    "from random import seed, shuffle\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "SEED = 1122\n",
    "seed(SEED) # set the random seed so that the random permutations can be reproduced again\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The adult dataset can be obtained from: http://archive.ics.uci.edu/ml/datasets/Adult\n",
    "    The code will look for the data files (adult.data, adult.test) in the present directory, if they are not found, it will download them from UCI archive.\n",
    "\"\"\"\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\") # get the current directory listing\n",
    "    print(\"Looking for file '%s' in the current directory...\" % fname)\n",
    "\n",
    "    if fname not in files:\n",
    "        print(\"'%s' not found! Downloading from UCI Archive...\" % fname)\n",
    "        addr = \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/%s\" % fname\n",
    "        response = urllib.request.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"w\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print(\"'%s' download and saved locally..\" % fname)\n",
    "    else:\n",
    "        print(\"File found in current directory..\")\n",
    "    \n",
    "    print()\n",
    "    return\n",
    "\n",
    "        \n",
    "def load_adult_data(load_data_size=None):\n",
    "\n",
    "    \"\"\"\n",
    "        if load_data_size is set to None (or if no argument is provided), then we load and return the whole data\n",
    "        if it is a number, say 10000, then we will return randomly selected 10K examples\n",
    "    \"\"\"\n",
    "\n",
    "    attrs = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country'] # all attributes\n",
    "    int_attrs = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'] # attributes with integer values -- the rest are categorical\n",
    "    sensitive_attrs = ['sex'] # the fairness constraints will be used for this feature\n",
    "    attrs_to_ignore = ['race', 'sex' ,'fnlwgt'] # sex is the sensitive feature so we will not use it in classification, we will not consider fnlwght for classification since its computed externally and it highly predictive for the class (for details, see documentation of the adult data)\n",
    "    attrs_for_classification = set(attrs) - set(attrs_to_ignore)\n",
    "\n",
    "    # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    data_files = [\"adult.data\", \"adult.test\"]\n",
    "    #data_files = [\"adult.test\"]\n",
    "\n",
    "\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    x_control = {}\n",
    "\n",
    "    attrs_to_vals = {} # will store the values for each attribute for all users\n",
    "    for k in attrs:\n",
    "        if k in sensitive_attrs:\n",
    "            x_control[k] = []\n",
    "        elif k in attrs_to_ignore:\n",
    "            pass\n",
    "        else:\n",
    "            attrs_to_vals[k] = []\n",
    "\n",
    "    for f in data_files:\n",
    "        check_data_file(f)\n",
    "\n",
    "        for line in open('./dataset/adult/' + f):\n",
    "            line = line.strip()\n",
    "            if line == \"\": continue # skip empty lines\n",
    "            line = line.split(\", \")\n",
    "            if len(line) != 15 or \"?\" in line: # if a line has missing attributes, ignore it\n",
    "                continue\n",
    "\n",
    "            class_label = line[-1]\n",
    "            if class_label in [\"<=50K.\", \"<=50K\"]:\n",
    "                class_label = -1\n",
    "            elif class_label in [\">50K.\", \">50K\"]:\n",
    "                class_label = +1\n",
    "            else:\n",
    "                raise Exception(\"Invalid class label value\")\n",
    "\n",
    "            y.append(class_label)\n",
    "\n",
    "\n",
    "            for i in range(0,len(line)-1):\n",
    "                attr_name = attrs[i]\n",
    "                attr_val = line[i]\n",
    "                # reducing dimensionality of some very sparse features\n",
    "                if attr_name == \"native_country\":\n",
    "                    if attr_val!=\"United-States\":\n",
    "                        attr_val = \"Non-United-Stated\"\n",
    "                elif attr_name == \"education\":\n",
    "                    if attr_val in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"]:\n",
    "                        attr_val = \"prim-middle-school\"\n",
    "                    elif attr_val in [\"9th\", \"10th\", \"11th\", \"12th\"]:\n",
    "                        attr_val = \"high-school\"\n",
    "\n",
    "                if attr_name in sensitive_attrs:\n",
    "                    x_control[attr_name].append(attr_val)\n",
    "                elif attr_name in attrs_to_ignore:\n",
    "                    pass\n",
    "                else:\n",
    "                    attrs_to_vals[attr_name].append(attr_val)\n",
    "\n",
    "    def convert_attrs_to_ints(d): # discretize the string attributes\n",
    "        for attr_name, attr_vals in list(d.items()):\n",
    "            if attr_name in int_attrs: continue\n",
    "            uniq_vals = sorted(list(set(attr_vals))) # get unique values\n",
    "\n",
    "            # compute integer codes for the unique values\n",
    "            val_dict = {}\n",
    "            for i in range(0,len(uniq_vals)):\n",
    "                val_dict[uniq_vals[i]] = i\n",
    "\n",
    "            # replace the values with their integer encoding\n",
    "            for i in range(0,len(attr_vals)):\n",
    "                attr_vals[i] = val_dict[attr_vals[i]]\n",
    "            d[attr_name] = attr_vals\n",
    "\n",
    "    \n",
    "    # convert the discrete values to their integer representations\n",
    "    convert_attrs_to_ints(x_control)\n",
    "    convert_attrs_to_ints(attrs_to_vals)\n",
    "\n",
    "\n",
    "    # if the integer vals are not binary, we need to get one-hot encoding for them\n",
    "\n",
    "    for attr_name in attrs_for_classification:\n",
    "\n",
    "        attr_vals = attrs_to_vals[attr_name]\n",
    "\n",
    "        if attr_name in int_attrs or attr_name == \"native_country\": \n",
    "            # the way we encoded native country, its binary now so no need to apply one hot encoding on it\n",
    "            X.append(attr_vals)\n",
    "            \n",
    "        else:\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            attr_vals = lb.fit_transform(attr_vals).T.tolist()\n",
    "            for bin_val_arr in attr_vals: # each binarized array of size n (n = num examples in the dataset)\n",
    "                X.append(bin_val_arr)\n",
    "            \n",
    "\n",
    "    # convert to numpy arrays for easy handline\n",
    "    X = np.array(X, dtype=float).T\n",
    "    y = np.array(y, dtype = float)\n",
    "    for k, v in list(x_control.items()): x_control[k] = np.array(v, dtype=float)\n",
    "        \n",
    "    # shuffle the data\n",
    "    perm = list(range(0,len(y))) # shuffle the data before creating each fold\n",
    "    shuffle(perm)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    for k in list(x_control.keys()):\n",
    "        x_control[k] = x_control[k][perm]\n",
    "\n",
    "    # see if we need to subsample the data\n",
    "    if load_data_size is not None:\n",
    "        print(\"Loading only %d examples from the data\" % load_data_size)\n",
    "        X = X[:load_data_size]\n",
    "        y = y[:load_data_size]\n",
    "        for k in list(x_control.keys()):\n",
    "            x_control[k] = x_control[k][:load_data_size]\n",
    "\n",
    "    x_sensitive = x_control[\"sex\"]\n",
    "    # np.savez(\"adult\", X, y, x_sensitive)\n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file 'adult.data' in the current directory...\n",
      "File found in current directory..\n",
      "\n",
      "Looking for file 'adult.test' in the current directory...\n",
      "File found in current directory..\n",
      "\n",
      "Different values of the sensible feature 9 : [0.0, 1.0]\n",
      "sensible_feature_values= [0.0, 1.0]\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. 35. 40.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0. 11.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 40. 42.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  9.]]\n"
     ]
    }
   ],
   "source": [
    "X, y, x_control = load_adult_data()\n",
    "#entry_count = 1000\n",
    "#X = X[:entry_count,]\n",
    "#y = y[:entry_count]\n",
    "#x_control = x_control[:entry_count]\n",
    "\n",
    "\n",
    "sensible_feature = 9  # sex\n",
    "sensible_feature_values = sorted(list(set(x_control)))\n",
    "print('Different values of the sensible feature', sensible_feature, ':', sensible_feature_values)\n",
    "ntrain = len(x_control)\n",
    "\n",
    "print(\"sensible_feature_values=\",sensible_feature_values)\n",
    "\n",
    "print(X[:2,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_control.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel_cust(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def linear_kernel(**kwargs):\n",
    "    def f(x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "    return f\n",
    "\n",
    "\n",
    "def polynomial_kernel(power, coef, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        return (np.dot(x1, x2) + coef)**power\n",
    "    return f\n",
    "\n",
    "\n",
    "def rbf_kernel(gamma, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        distance = np.linalg.norm(x1 - x2) ** 2\n",
    "        return np.exp(-gamma * distance)\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=3,With Feature\n",
      "P.shape (670, 670)\n",
      "q.shape (670, 1)\n",
      "G.shape (1340, 670)\n",
      "h.shape (1340, 1)\n",
      "A.shape (1, 670)\n",
      "b.shape (1, 1)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.5907e+02 -5.4898e+03  1e+04  7e-01  3e-15\n",
      " 1: -5.3018e+02 -1.8450e+03  1e+03  3e-15  4e-15\n",
      " 2: -6.0554e+02 -9.3770e+02  3e+02  1e-14  4e-15\n",
      " 3: -6.4608e+02 -7.3790e+02  9e+01  3e-14  4e-15\n",
      " 4: -6.5878e+02 -6.8449e+02  3e+01  2e-14  4e-15\n",
      " 5: -6.6339e+02 -6.6866e+02  5e+00  1e-14  4e-15\n",
      " 6: -6.6449e+02 -6.6515e+02  7e-01  1e-14  4e-15\n",
      " 7: -6.6466e+02 -6.6469e+02  4e-02  2e-14  4e-15\n",
      " 8: -6.6467e+02 -6.6467e+02  2e-03  1e-14  4e-15\n",
      " 9: -6.6467e+02 -6.6467e+02  6e-05  3e-14  4e-15\n",
      "Optimal solution found.\n",
      "Accuracy: 0.7303030303030303\n",
      "=====================================================\n",
      "C=3,With Feature\n",
      "P.shape (670, 670)\n",
      "q.shape (670, 1)\n",
      "G.shape (1340, 670)\n",
      "h.shape (1340, 1)\n",
      "A.shape (2, 670)\n",
      "b.shape (2, 1)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.5815e+02 -5.4737e+03  1e+04  7e-01  3e-15\n",
      " 1: -5.2846e+02 -1.8362e+03  1e+03  3e-14  3e-15\n",
      " 2: -6.0269e+02 -9.3216e+02  3e+02  2e-14  3e-15\n",
      " 3: -6.4320e+02 -7.3432e+02  9e+01  3e-14  3e-15\n",
      " 4: -6.5596e+02 -6.8102e+02  3e+01  3e-14  4e-15\n",
      " 5: -6.6048e+02 -6.6578e+02  5e+00  1e-14  4e-15\n",
      " 6: -6.6158e+02 -6.6227e+02  7e-01  8e-14  4e-15\n",
      " 7: -6.6175e+02 -6.6179e+02  4e-02  5e-14  4e-15\n",
      " 8: -6.6176e+02 -6.6177e+02  3e-03  2e-15  4e-15\n",
      " 9: -6.6176e+02 -6.6176e+02  2e-04  2e-14  4e-15\n",
      "Optimal solution found.\n",
      "Accuracy: 0.6424242424242425\n",
      "=====================================================\n",
      "C=10,With Feature\n",
      "P.shape (670, 670)\n",
      "q.shape (670, 1)\n",
      "G.shape (1340, 670)\n",
      "h.shape (1340, 1)\n",
      "A.shape (1, 670)\n",
      "b.shape (1, 1)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4724e+02 -3.0331e+04  4e+04  2e-01  1e-14\n",
      " 1: -8.6341e+02 -5.6866e+03  5e+03  2e-02  1e-14\n",
      " 2: -1.3248e+03 -2.5604e+03  1e+03  3e-03  1e-14\n",
      " 3: -1.5045e+03 -1.9023e+03  4e+02  4e-04  1e-14\n",
      " 4: -1.5658e+03 -1.6571e+03  9e+01  1e-13  1e-14\n",
      " 5: -1.5808e+03 -1.5939e+03  1e+01  2e-14  1e-14\n",
      " 6: -1.5834e+03 -1.5849e+03  1e+00  4e-14  1e-14\n",
      " 7: -1.5838e+03 -1.5839e+03  9e-02  2e-14  1e-14\n",
      " 8: -1.5838e+03 -1.5838e+03  2e-03  1e-13  1e-14\n",
      " 9: -1.5838e+03 -1.5838e+03  9e-05  5e-15  1e-14\n",
      "Optimal solution found.\n",
      "Accuracy: 0.7121212121212122\n",
      "=====================================================\n",
      "C=10,With Feature\n",
      "P.shape (670, 670)\n",
      "q.shape (670, 1)\n",
      "G.shape (1340, 670)\n",
      "h.shape (1340, 1)\n",
      "A.shape (2, 670)\n",
      "b.shape (2, 1)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.5735e+02 -3.0297e+04  4e+04  2e-01  1e-14\n",
      " 1: -8.6549e+02 -5.8271e+03  5e+03  2e-02  1e-14\n",
      " 2: -1.3195e+03 -2.5731e+03  1e+03  3e-03  1e-14\n",
      " 3: -1.5025e+03 -1.9036e+03  4e+02  4e-04  1e-14\n",
      " 4: -1.5641e+03 -1.6575e+03  9e+01  2e-13  1e-14\n",
      " 5: -1.5794e+03 -1.5928e+03  1e+01  1e-13  1e-14\n",
      " 6: -1.5821e+03 -1.5835e+03  1e+00  4e-14  1e-14\n",
      " 7: -1.5825e+03 -1.5825e+03  9e-02  8e-14  1e-14\n",
      " 8: -1.5825e+03 -1.5825e+03  2e-03  5e-15  1e-14\n",
      " 9: -1.5825e+03 -1.5825e+03  9e-05  6e-14  1e-14\n",
      "Optimal solution found.\n",
      "Accuracy: 0.7090909090909091\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_control_train, X_control_test, y_train, y_test = train_test_split(x_control, y, test_size=0.33)\n",
    "\n",
    "for C in [3,10]:\n",
    "    \n",
    "    for feature in [None, x_control]:\n",
    "        p_feature = None\n",
    "        if (x_control is not None):\n",
    "            p_feature = 'With Feature'\n",
    "        print(f'C={C},{p_feature}')\n",
    "        clf = SupportVectorMachine(kernel=rbf_kernel, sensible_feature=feature, C=C, power=4, coef=1)\n",
    "        #clf = SupportVectorMachine(kernel=polynomial_kernel, sensible_feature=x_control)\n",
    "        clf.fit(X_train, y_train)\n",
    "        #clf.fit(X, y)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        #y_pred = clf.predict(X)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        #accuracy = accuracy_score(y, y_pred)\n",
    "        print (\"Accuracy:\", accuracy)\n",
    "        print(\"=====================================================\")\n",
    "        # Reduce dimension to two using PCA and plot the results\n",
    "        #Plot().plot_in_2d(X, y_pred, title=\"Support Vector Machine\", accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of topics for the final project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 algorithmic fairness\n",
    "Algorithm fairness is becoming a fundamental topic in ML. It is a complex ethical task to define what fairness is/means. Once we have defined quantitatively what fairness is then, from a mathematical perspective, the problem of  algorithmic fairness is very clear: it is a multi-objective optimization problem. There are multi objectives that we aim to optimize during data fitting, e.g., accuracy and fairness.\n",
    "\n",
    "The goal of this project is to implement from scratch the \"fair\" **linear and nonlinear SVM** described in the following paper (see in particular Appendix A that reports the optimization problem)  \n",
    "\n",
    "[\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf)\n",
    "\n",
    "and reproduce the experiments reported in the paper. In particular, apply your method to the Adult and Bank \n",
    "data sets.\n",
    "\n",
    "Your notebook must include:\n",
    "* a description (summary) of the algorithm presented in the above paper (focusing on SVM), similar to the theoretical details of logistic regression I wrote at the beginning of the notebook for e-tivity Task A (week 1&2). The reader must understand from your explanation the difference between standard SVM and the \"fair\" SVM.\n",
    "* You implementation of the \"fair\" **linear and nonlinear SVM** using CVXOPT to solve data fitting (as I have shown in Week 3 webinar, see also example below). You should implement it as a Python class (similar to logistic regression class for E-tivity 1).\n",
    "* A test of the input-output behavior of your algorithm. More clearly, you have to replicate the experiment results you find in Section 4.1 for Synthetic Data and Section 4.2 of the above paper for the Adult and Bank data sets.\n",
    "\n",
    "\n",
    "Resources:\n",
    "* Week 3 webinar slides with the details of the SVM algorithm;\n",
    "* [Example](https://xavierbourretsicotte.github.io/SVM_implementation.html) about how to use the library CVXOPT to implement data fitting for standard SVM\n",
    "* [fairness-in-machine-learning](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)\n",
    "* (Optional) Multi-objective optimization and Pareto optimality see Book chapter 12 (of our Module's book).\n",
    "\n",
    "**How to approach the problem** (this is just a suggestion).\n",
    "\n",
    "You can start implementing linear SVM and apply it to the Synthetic Data experiment in Section 4.1 so that you can plot the classification line for standard linear SVM versus fair linear SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:45.225887Z",
     "start_time": "2020-02-15T12:53:29.993350Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import svm\n",
    "import cvxopt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## for synthetic data\n",
    "import math\n",
    "import matplotlib.pyplot as plt # for plotting stuff\n",
    "from random import seed, shuffle\n",
    "from scipy.stats import multivariate_normal # generating synthetic data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(**kwargs):\n",
    "    def f(x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "    return f\n",
    "\n",
    "\n",
    "def polynomial_kernel(power, coef, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        return (np.dot(x1, x2) + coef)**power\n",
    "    return f\n",
    "\n",
    "\n",
    "def rbf_kernel(gamma, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        distance = np.linalg.norm(x1 - x2) ** 2\n",
    "        return np.exp(-gamma * distance)\n",
    "    return f\n",
    "\n",
    "def compute_p_rule(print_string, x_control, class_labels):\n",
    "\n",
    "    try:\n",
    "        \"\"\" Compute the p-rule based on Doctrine of disparate impact \"\"\"\n",
    "        non_prot_all = sum(x_control == 1.0) # non-protected group\n",
    "        prot_all = sum(x_control == 0.0) # protected group\n",
    "        non_prot_pos = sum(class_labels[x_control == 1.0] == 1.0) # non_protected in positive class\n",
    "        prot_pos = sum(class_labels[x_control == 0.0] == 1.0) # protected in positive class\n",
    "        frac_non_prot_pos = float(non_prot_pos) / float(non_prot_all)\n",
    "        frac_prot_pos = float(prot_pos) / float(prot_all)\n",
    "        p_rule = (frac_prot_pos / frac_non_prot_pos) * 100.0\n",
    "        #print ()\n",
    "        #print((\"Total data points: %d\" % (len(x_control))))\n",
    "        #print((\"# non-protected examples: %d\" % (non_prot_all)))\n",
    "        #print((\"# protected examples: %d\" % (prot_all)))\n",
    "        #print((\"Non-protected in positive class: %d (%0.0f%%)\" % (non_prot_pos, non_prot_pos * 100.0 / non_prot_all)))\n",
    "        #print((\"Protected in positive class: %d (%0.0f%%)\" % (prot_pos, prot_pos * 100.0 / prot_all)))\n",
    "        print((\"P-rule is: %0.0f%%\" % ( p_rule )))\n",
    "        return p_rule\n",
    "    except ZeroDivisionError:\n",
    "        print(\"p_rule divided by 0\")\n",
    "    \n",
    "    return np.NaN\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide cvxopt output\n",
    "cvxopt.solvers.options['show_progress'] = True\n",
    "cvxopt.solvers.options['abstol'] = 1e-10\n",
    "cvxopt.solvers.options['reltol'] = 1e-10\n",
    "cvxopt.solvers.options['feastol'] = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:48.097038Z",
     "start_time": "2020-02-15T12:53:47.888157Z"
    }
   },
   "outputs": [],
   "source": [
    "class SupportVectorMachine(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=None, kernel=rbf_kernel, sensible_feature=None, power=4, gamma=None, coef=4, correlation=0.0):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature   \n",
    "        self.correlation = correlation\n",
    "        self.lagrang_limiter = 1e-4\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        if self.fairness:\n",
    "            self.values_of_sensible_feature = list(set(self.sensible_feature))\n",
    "            self.list_of_sensible_feature_train = self.sensible_feature\n",
    "            self.val0 = np.min(self.values_of_sensible_feature)\n",
    "            self.val1 = np.max(self.values_of_sensible_feature)\n",
    "            self.set_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                           and self.sensible_feature[idx] == self.val1]\n",
    "            self.set_not_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                               and self.sensible_feature[idx] == self.val0]\n",
    "            self.set_1 = [idx for idx, ex in enumerate(X) if y[idx] == 1]\n",
    "            self.n_A1 = len(self.set_A1)\n",
    "            self.n_not_A1 = len(self.set_not_A1)\n",
    "            self.n_1 = len(self.set_1)  \n",
    "            \n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if not self.C:\n",
    "            G = cvxopt.matrix(np.identity(n_samples) * -1)\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            G_max = np.identity(n_samples) * -1\n",
    "            G_min = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n",
    "            h_max = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n",
    "            \n",
    "        # Stack the fairness constraint\n",
    "        if self.fairness:\n",
    "            tau = [(np.sum(kernel_matrix[self.set_A1, idx]) / self.n_A1) - (np.sum(kernel_matrix[self.set_not_A1, idx]) / self.n_not_A1) for idx in range(len(y))]\n",
    "            fairness_line = cvxopt.matrix(y * tau, (1, n_samples), 'd')\n",
    "            A = cvxopt.matrix(np.vstack([A, fairness_line]))\n",
    "            b = cvxopt.matrix([0.0, self.correlation])            \n",
    "            \n",
    "        #print(\"P.shape\", P.size)\n",
    "        #print(\"q.shape\", q.size)\n",
    "        #print(\"G.shape\", G.size)\n",
    "        #print(\"h.shape\", h.size)\n",
    "        #print(\"A.shape\", A.size)\n",
    "        #print(\"b.shape\", b.size)\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        \n",
    "        \n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "        #self.all_multipliers = lagr_mult\n",
    "        #w parameter in vectorized form\n",
    "        #self.w = ((y * self.all_multipliers).T @ X).reshape(-1,1)\n",
    "        \n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > self.lagrang_limiter\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "        \n",
    "        w_tmp = ((self.support_vector_labels * self.lagr_multipliers).T @ self.support_vectors).reshape(-1,1)\n",
    "        self.w = [w_tmp[0,0], w_tmp[1,0]]\n",
    "\n",
    "        #Selecting the set of indices S corresponding to non zero parameters\n",
    "        #S = (self.lagr_multipliers > self.lagrang_limiter).flatten()\n",
    "        b = y[idx].reshape(-1,1) - np.dot(X[idx], self.w)\n",
    "        self.b=b[0,0]\n",
    "        \n",
    "        #Computing b\n",
    "        #b = yfit[S] - np.dot(xfit[S], w)\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "def plot_decision_boundary(X, y, w,b,xfit,feature):\n",
    "\n",
    "    print(f'w={w},b={b[0].dtype},feature={feature}')\n",
    "    #print(f'{w[0][0]}, {w[0][1]}, {w[1][0]}, {w[1][1]}, {b[0]}, {b[1]}')\n",
    "    print(-w[0][0] * xfit)\n",
    "    print((-w[0][0] * xfit-b[0])/w[0][1])\n",
    "    if (feature[0] is not None):\n",
    "        line = '--b'\n",
    "    else:\n",
    "        line = '-k'\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plt.plot(xfit, (-np.array(w[0][0]) * xfit-b[0])/np.array(w[0][1]), line)\n",
    "    plt.plot(xfit, (-np.array(w[1][0]) * xfit-b[1])/np.array(w[1][1]), line)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorMachine_robin(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=1, kernel=rbf_kernel, power=4, gamma=None, coef=4, sensible_feature=None, correlation=0.0):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature  \n",
    "        self.c = correlation\n",
    "        self.lagrang_limiter=1e-4\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        # Define the quadratic optimization problem\n",
    "        if self.fairness:\n",
    "            P = cvxopt.matrix(((kernel_matrix+(1/self.C)*(np.eye(n_samples)))*(y.dot(y.T))), tc='d')\n",
    "        else:\n",
    "            P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d') \n",
    "            \n",
    "        q = cvxopt.matrix(-np.ones(n_samples))\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if self.fairness and self.C:\n",
    "            print('Inside if self.fairness and self.C:')\n",
    "            G_1 = np.identity(n_samples) * -1\n",
    "            G_2 = (1/n_samples)*(kernel_matrix*np.dot(self.sensible_feature,y.T))\n",
    "            G_3 = - (1/n_samples)*(kernel_matrix*np.dot(self.sensible_feature,y.T))\n",
    "            G_4 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((G_1, G_2, G_3, G_4)))\n",
    "            h_1 = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_2 = cvxopt.matrix(np.ones(n_samples) * self.c)\n",
    "            h_3 = cvxopt.matrix(np.ones(n_samples) * self.c)\n",
    "            h_4 = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            h = cvxopt.matrix(np.vstack((h_1, h_2, h_3, h_4)))\n",
    "        elif self.C and not self.fairness:\n",
    "            print('elif self.C and not self.fairness:')\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1,np.identity(n_samples))), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * self.C)), tc='d')\n",
    "        else:\n",
    "            print('Else')\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1)), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack((np.zeros(n_samples))), tc='d')\n",
    "            \n",
    "        print(\"P.shape\", P.size)\n",
    "        print(\"q.shape\", q.size)\n",
    "        print(\"G.shape\", G.size)\n",
    "        print(\"h.shape\", h.size)\n",
    "        print(\"A.shape\", A.size)\n",
    "        print(\"b.shape\", b.size)\n",
    "        print(\"c\", self.c)\n",
    "        print(\"C\", self.C)\n",
    "        \n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "        self.all_multipliers = lagr_mult\n",
    "        #w parameter in vectorized form\n",
    "        self.w = ((y * self.all_multipliers).T @ X).reshape(-1,1)\n",
    "        \n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > self.lagrang_limiter\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "        \n",
    "        b = y[idx].reshape(-1,1) - np.dot(X[idx], self.w)\n",
    "        self.b=b[0,0]\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0, './fair-classification3/fair_classification') # the code for fair classification is in this directory\n",
    "\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import numpy as np\n",
    "from random import seed, shuffle\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "SEED = 1122\n",
    "seed(SEED) # set the random seed so that the random permutations can be reproduced again\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic data\n",
    "def load_synthetic_data(load_data_size=None, drop_sensitive=True, plot_data=False):\n",
    "\n",
    "    \"\"\"\n",
    "        Code for generating the synthetic data.\n",
    "        We will have two non-sensitive features and one sensitive feature.\n",
    "        A sensitive feature value of 0.0 means the example is considered to be in protected group (e.g., female) and 1.0 means it's in non-protected group (e.g., male).\n",
    "    \"\"\"\n",
    "\n",
    "    if (load_data_size == None):\n",
    "        n_samples = 1000 # generate these many data points per class\n",
    "    else:\n",
    "        n_samples = load_data_size\n",
    "        \n",
    "    disc_factor = math.pi / 4.0 # this variable determines the initial discrimination in the data -- decraese it to generate more discrimination\n",
    "\n",
    "    def gen_gaussian(mean_in, cov_in, class_label):\n",
    "        nv = multivariate_normal(mean = mean_in, cov = cov_in)\n",
    "        X = nv.rvs(n_samples)\n",
    "        y = np.ones(n_samples, dtype=float) * class_label\n",
    "        return nv,X,y\n",
    "\n",
    "    \"\"\" Generate the non-sensitive features randomly \"\"\"\n",
    "    # We will generate one gaussian cluster for each class\n",
    "    mu1, sigma1 = [2, 2], [[5, 1], [1, 5]]\n",
    "    mu2, sigma2 = [-2,-2], [[10, 1], [1, 3]]\n",
    "    nv1, X1, y1 = gen_gaussian(mu1, sigma1, 1) # positive class\n",
    "    nv2, X2, y2 = gen_gaussian(mu2, sigma2, -1) # negative class\n",
    "\n",
    "    # join the posisitve and negative class clusters\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.hstack((y1, y2))\n",
    "\n",
    "    # shuffle the data\n",
    "    perm = list(range(0,n_samples*2))\n",
    "    shuffle(perm)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    rotation_mult = np.array([[math.cos(disc_factor), -math.sin(disc_factor)], [math.sin(disc_factor), math.cos(disc_factor)]])\n",
    "    X_aux = np.dot(X, rotation_mult)\n",
    "\n",
    "\n",
    "    \"\"\" Generate the sensitive feature here \"\"\"\n",
    "    x_control = [] # this array holds the sensitive feature value\n",
    "    for i in range (0, len(X)):\n",
    "        x = X_aux[i]\n",
    "\n",
    "        # probability for each cluster that the point belongs to it\n",
    "        p1 = nv1.pdf(x)\n",
    "        p2 = nv2.pdf(x)\n",
    "        \n",
    "        # normalize the probabilities from 0 to 1\n",
    "        s = p1+p2\n",
    "        p1 = p1/s\n",
    "        p2 = p2/s\n",
    "        \n",
    "        r = np.random.uniform() # generate a random number from 0 to 1\n",
    "\n",
    "        if r < p1: # the first cluster is the positive class\n",
    "            x_control.append(1.0) # 1.0 means its male\n",
    "        else:\n",
    "            x_control.append(0.0) # 0.0 -> female\n",
    "\n",
    "    x_control = np.array(x_control)\n",
    "\n",
    "    \"\"\" Show the data \"\"\"\n",
    "    if plot_data:\n",
    "        num_to_draw = 200 # we will only draw a small number of points to avoid clutter\n",
    "        x_draw = X[:num_to_draw]\n",
    "        y_draw = y[:num_to_draw]\n",
    "        x_control_draw = x_control[:num_to_draw]\n",
    "\n",
    "        X_s_0 = x_draw[x_control_draw == 0.0]\n",
    "        X_s_1 = x_draw[x_control_draw == 1.0]\n",
    "        y_s_0 = y_draw[x_control_draw == 0.0]\n",
    "        y_s_1 = y_draw[x_control_draw == 1.0]\n",
    "        plt.scatter(X_s_0[y_s_0==1.0][:, 0], X_s_0[y_s_0==1.0][:, 1], color='green', marker='x', s=30, linewidth=1.5, label= \"Prot. +ve\")\n",
    "        plt.scatter(X_s_0[y_s_0==-1.0][:, 0], X_s_0[y_s_0==-1.0][:, 1], color='red', marker='x', s=30, linewidth=1.5, label = \"Prot. -ve\")\n",
    "        plt.scatter(X_s_1[y_s_1==1.0][:, 0], X_s_1[y_s_1==1.0][:, 1], color='green', marker='o', facecolors='none', s=30, label = \"Non-prot. +ve\")\n",
    "        plt.scatter(X_s_1[y_s_1==-1.0][:, 0], X_s_1[y_s_1==-1.0][:, 1], color='red', marker='o', facecolors='none', s=30, label = \"Non-prot. -ve\")\n",
    "\n",
    "        \n",
    "        plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off') # dont need the ticks to see the data distribution\n",
    "        plt.tick_params(axis='y', which='both', left='off', right='off', labelleft='off')\n",
    "        plt.legend(loc=2, fontsize=15)\n",
    "        plt.xlim((-15,10))\n",
    "        plt.ylim((-10,15))\n",
    "        #plt.savefig(\"img/data.png\")\n",
    "        plt.show()\n",
    "\n",
    "    #x_control = {\"s1\": x_control} # all the sensitive features are stored in a dictionary\n",
    "    print(f\"Shapes of synthetic X = {X.shape}, y={y.shape}, x_sensitive={x_control.shape}\")\n",
    "    return X,y,x_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the adult data        \n",
    "def load_adult_data(load_data_size=None, drop_sensitive=True):\n",
    "\n",
    "   # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    attrs = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country','target'] \n",
    "    \n",
    "    data = pd.read_csv(\"dataset/adult/adult.data\", names=attrs, skipinitialspace=True) \n",
    "\n",
    "    data = data.replace(to_replace ='?', value =np.NaN)\n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    target_mapping = {'<=50K': -1, '>50K': 1,'<=50K.': -1, '>50K.': 1}\n",
    "    data.replace({\"target\": target_mapping}, inplace=True)\n",
    "    data['native_country'] = data['native_country'].apply(lambda x: 'USA' if x in ['United-States'] else 'NON-USA')\n",
    "    data['education'] = data['education'].apply(lambda x: 'pre-middle-school' if x in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"] else 'high-school')\n",
    "\n",
    "    encoded_object_df = pd.DataFrame()\n",
    "\n",
    "    for column in ['workclass', 'sex', 'education', 'marital_status', 'occupation','relationship',  'native_country']:\n",
    "        # race\n",
    "        encoded_object_df = pd.concat([encoded_object_df,pd.get_dummies(data[column], prefix=column, drop_first=True)] ,axis=1)\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    cols_to_scale = ['age', 'education_num', 'capital_gain', 'capital_loss', 'capital_loss', 'target']\n",
    "    #fnlwgt\n",
    "\n",
    "    encoded_int_df = data[cols_to_scale]\n",
    "\n",
    "    encoded_int_df[cols_to_scale] = min_max_scaler.fit_transform(encoded_int_df[cols_to_scale])\n",
    "    \n",
    "    final_df = pd.concat([encoded_object_df, encoded_int_df], axis=1)\n",
    "    #final_df = final_df.sample(n=load_data_size)\n",
    "    final_df = final_df.iloc[0:load_data_size]\n",
    "    \n",
    "    # shuffle the data\n",
    "    \n",
    "    y = np.where(final_df['target'] == 0, -1, 1)\n",
    "    x_sensitive = np.array(final_df['sex_Male'])\n",
    "    \n",
    "    if drop_sensitive:\n",
    "        final_df.drop(['sex_Male'], axis=1, inplace=True)\n",
    "        \n",
    "    final_df.drop(['target'], axis=1, inplace=True)\n",
    "        \n",
    "    X = np.array(final_df)\n",
    "    \n",
    "    print(f\"Shapes of adult X = {X.shape}, y={y.shape}, x_sensitive={x_sensitive.shape}\")\n",
    "    \n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load bank data\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_bank_data(load_data_size=None, drop_sensitive=True):\n",
    "\n",
    "   # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    attrs = [\"age\",\"job\",\"marital\",\"education\",\"default\",\"balance\",\"housing\",\"loan\",\"contact\",\"day\",\"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\",\"y\"] \n",
    "    \n",
    "    data = pd.read_csv(\"dataset/bank/bank.csv\", skipinitialspace=True, delimiter=';') \n",
    "\n",
    "    data['month-seasonal'] = data['month'].apply(lambda x: 'q1' if x in [\"jan\", \"feb\", \"mar\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q2' if x in [\"apr\", \"may\", \"jun\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q3' if x in [\"jul\", \"aug\", \"sep\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q4' if x in [\"oct\", \"nov\", \"dec\"] else x )\n",
    "    data['age-range'] = data['age'].apply(lambda x: 0.0 if (x >=25 and x<=60)  else 1.0 )\n",
    "\n",
    "    data.drop(['month'], inplace=True, axis=1)\n",
    "    \n",
    "    encoded_object_df = pd.DataFrame()\n",
    "\n",
    "    for column in ['job', 'marital', 'education', 'default', 'housing','loan','contact', 'poutcome','y','month-seasonal']:\n",
    "        encoded_object_df = pd.concat([encoded_object_df,pd.get_dummies(data[column], prefix=column, drop_first=True)] ,axis=1)\n",
    "    \n",
    "    min_max_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    cols_to_scale = ['balance','day','duration', 'campaign', 'pdays','previous']\n",
    "\n",
    "    encoded_int_df = data[cols_to_scale]\n",
    "\n",
    "    encoded_int_df[cols_to_scale] = min_max_scaler.fit_transform(encoded_int_df[cols_to_scale])\n",
    "    \n",
    "    final_df = pd.concat([encoded_object_df, encoded_int_df, data['age-range']], axis=1)\n",
    "    \n",
    "    #final_df = final_df.sample(n=load_data_size, random_state=1234)\n",
    "    final_df = final_df.iloc[0:load_data_size]\n",
    "    \n",
    "    y = np.where(final_df['y_yes'] == 0, -1, 1)\n",
    "    x_sensitive = np.array(final_df['age-range'])\n",
    "\n",
    "    \n",
    "    if drop_sensitive:\n",
    "        final_df.drop(['age-range'], axis=1, inplace=True)\n",
    "        \n",
    "    final_df.drop(['y_yes'], axis=1, inplace=True)\n",
    "        \n",
    "    X = np.array(final_df)\n",
    "    \n",
    "    print(f\"Shapes of bank X = {X.shape}, y={y.shape}, x_sensitive={x_sensitive.shape}\")\n",
    "    \n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cov_thresh_vs_acc_pos_ratio(x_all, y_all, x_control_all, num_folds, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, sensitive_attrs):\n",
    "\n",
    "\n",
    "    # very the covariance threshold using a range of decreasing multiplicative factors and see the tradeoffs between accuracy and fairness\n",
    "    it = 0.05\n",
    "    cov_range = np.arange(1.0, 0.0-it, -it).tolist()\n",
    "    if apply_accuracy_constraint == True:\n",
    "        if sep_constraint == False:\n",
    "            it = 0.1\n",
    "            cov_range = np.arange(0.0, 1.0 + it, it).tolist()\n",
    "        if sep_constraint == True:\n",
    "            cov_range =  [0,1,5,10,20,50,100,500,1000]\n",
    "\n",
    "    \n",
    "    positive_class_label = 1 # positive class is +1\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    positive_per_category = defaultdict(list) # for each category (male / female), the frac of positive\n",
    "\n",
    "    # first get the original values of covariance in the unconstrained classifier -- these original values are not needed for reverse constraint    \n",
    "    test_acc_arr, train_acc_arr, correlation_dict_test_arr, correlation_dict_train_arr, cov_dict_test_arr, cov_dict_train_arr = compute_cross_validation_error(x_all, y_all, x_control_all, num_folds, loss_function, 0, apply_accuracy_constraint, sep_constraint, sensitive_attrs, [{} for i in range(0,num_folds)], 0)\n",
    "\n",
    "    for c in cov_range:\n",
    "        print((\"LOG: testing for multiplicative factor: %0.2f\" % c)) \n",
    "        sensitive_attrs_to_cov_original_arr_multiplied = []\n",
    "        for sensitive_attrs_to_cov_original in cov_dict_train_arr:\n",
    "            sensitive_attrs_to_cov_thresh = deepcopy(sensitive_attrs_to_cov_original)\n",
    "            for k in list(sensitive_attrs_to_cov_thresh.keys()):\n",
    "                v = sensitive_attrs_to_cov_thresh[k]\n",
    "                if type(v) == type({}):\n",
    "                    for k1 in list(v.keys()):\n",
    "                        v[k1] = v[k1] * c\n",
    "                else:\n",
    "                    sensitive_attrs_to_cov_thresh[k] = v * c\n",
    "            sensitive_attrs_to_cov_original_arr_multiplied.append(sensitive_attrs_to_cov_thresh)\n",
    "\n",
    "\n",
    "        test_acc_arr, train_acc_arr, correlation_dict_test_arr, correlation_dict_train_arr, cov_dict_test_arr, cov_dict_train_arr  = compute_cross_validation_error(x_all, y_all, x_control_all, num_folds, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, sensitive_attrs, sensitive_attrs_to_cov_original_arr_multiplied, c)\n",
    "        test_acc.append(np.mean(test_acc_arr))\n",
    "\n",
    "\n",
    "        correlation_dict_train = get_avg_correlation_dict(correlation_dict_train_arr)\n",
    "        correlation_dict_test = get_avg_correlation_dict(correlation_dict_test_arr)\n",
    "        \n",
    "        # just plot the correlations for the first sensitive attr, the plotting can be extended for the other values, but as a proof of concept, we will jsut show for one\n",
    "        s = sensitive_attrs[0]    \n",
    "        \n",
    "        for k,v in list(correlation_dict_test[s].items()):\n",
    "            if v.get(positive_class_label) is None:\n",
    "                positive_per_category[k].append(0.0)\n",
    "            else:\n",
    "                positive_per_category[k].append(v[positive_class_label])\n",
    "    \n",
    "    positive_per_category = dict(positive_per_category)\n",
    "    \n",
    "    p_rule_arr = (np.array(positive_per_category[0]) / np.array(positive_per_category[1])) * 100.0\n",
    "    \n",
    "\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    plt.plot(cov_range, positive_per_category[0], \"-o\" , color=\"green\", label = \"Protected\")\n",
    "    plt.plot(cov_range, positive_per_category[1], \"-o\", color=\"blue\", label = \"Non-protected\")\n",
    "    ax.set_xlim([min(cov_range), max(cov_range)])\n",
    "    plt.xlabel('Multiplicative loss factor')\n",
    "    plt.ylabel('Perc. in positive class')\n",
    "    if apply_accuracy_constraint == False:\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.xlabel('Multiplicative covariance factor (c)')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = plt.subplot(2,1,2)\n",
    "    plt.scatter(p_rule_arr, test_acc, color=\"red\")\n",
    "    ax.set_xlim([min(p_rule_arr), max(max(p_rule_arr), 100)])\n",
    "    plt.xlabel('P% rule')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def get_line_coordinates(w, x1, x2):\n",
    "    y1 = (-w[0] - (w[1] * x1)) / w[2]\n",
    "    y2 = (-w[0] - (w[1] * x2)) / w[2]    \n",
    "    return y1,y2\n",
    "\n",
    "    def plot_boundaries(w1, w2, p1, p2, acc1, acc2, fname):\n",
    "\n",
    "        num_to_draw = 200 # we will only draw a small number of points to avoid clutter\n",
    "        x_draw = X[:num_to_draw]\n",
    "        y_draw = y[:num_to_draw]\n",
    "        x_control_draw = x_control[\"s1\"][:num_to_draw]\n",
    "\n",
    "        X_s_0 = x_draw[x_control_draw == 0.0]\n",
    "        X_s_1 = x_draw[x_control_draw == 1.0]\n",
    "        y_s_0 = y_draw[x_control_draw == 0.0]\n",
    "        y_s_1 = y_draw[x_control_draw == 1.0]\n",
    "        plt.scatter(X_s_0[y_s_0==1.0][:, 1], X_s_0[y_s_0==1.0][:, 2], color='green', marker='x', s=30, linewidth=1.5)\n",
    "        plt.scatter(X_s_0[y_s_0==-1.0][:, 1], X_s_0[y_s_0==-1.0][:, 2], color='red', marker='x', s=30, linewidth=1.5)\n",
    "        plt.scatter(X_s_1[y_s_1==1.0][:, 1], X_s_1[y_s_1==1.0][:, 2], color='green', marker='o', facecolors='none', s=30)\n",
    "        plt.scatter(X_s_1[y_s_1==-1.0][:, 1], X_s_1[y_s_1==-1.0][:, 2], color='red', marker='o', facecolors='none', s=30)\n",
    "\n",
    "\n",
    "        x1,x2 = max(x_draw[:,1]), min(x_draw[:,1])\n",
    "        y1,y2 = ut.get_line_coordinates(w1, x1, x2)\n",
    "        plt.plot([x1,x2], [y1,y2], 'c-', linewidth=3, label = \"Acc=%0.2f; p%% rule=%0.0f%% - Original\"%(acc1, p1))\n",
    "        y1,y2 = ut.get_line_coordinates(w2, x1, x2)\n",
    "        plt.plot([x1,x2], [y1,y2], 'b--', linewidth=3, label = \"Acc=%0.2f; p%% rule=%0.0f%% - Constrained\"%(acc2, p2))\n",
    "\n",
    "\n",
    "\n",
    "        plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off') # dont need the ticks to see the data distribution\n",
    "        plt.tick_params(axis='y', which='both', left='off', right='off', labelleft='off')\n",
    "        plt.legend(loc=2, fontsize=15)\n",
    "        plt.xlim((-15,10))\n",
    "        plt.ylim((-10,15))\n",
    "        plt.savefig(fname)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "def get_data(data_type, sample_count=500):\n",
    "    if data_type == 'bank':\n",
    "        p_X, p_y, p_x_control = load_bank_data(load_data_size=sample_count)\n",
    "    elif (data_type == 'synthetic'):\n",
    "        p_X, p_y, p_x_control = load_synthetic_data(load_data_size=sample_count, plot_data=True)\n",
    "    else:\n",
    "        p_X, p_y, p_x_control = load_adult_data(load_data_size=sample_count)\n",
    "        \n",
    "    return p_X, p_y, p_x_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data={'data_type': [], 'samples(train)': [], 'constraints': [], 'accuracy' : [], 'p_rule' : [], 'accuracy_svm(no_constraints)' : [], 'kernel' : [], 'C': [], 'coeff_c': []}\n",
    "\n",
    "def run_experiment(X, y, x_control, C_values, data_type, kernel=rbf_kernel, correlation=0.0, results=results_data):\n",
    "       \n",
    "    ## compute the p_rule on base scenario\n",
    "    compute_p_rule(\"Base Data \" + data_type, x_control, y)\n",
    "    \n",
    "    w , b, xfit, f_feature=[],[],[],[]\n",
    "    \n",
    "    for C in C_values:\n",
    "        for feature in [None, x_control]:\n",
    "            p_feature = None\n",
    "            if (feature is not None):\n",
    "                p_feature = 'Fairness Constraints'\n",
    "            print(f'C={C},{p_feature}')\n",
    "            clf = SupportVectorMachine(kernel=kernel, sensible_feature=feature, C=C, power=4, coef=1, correlation=correlation)\n",
    "            #clf = SupportVectorMachine_trial(kernel=kernel, sensible_feature=feature, C=C, power=4, coef=1, correlation=correlation)\n",
    "            #clf = SupportVectorMachine_robin(kernel=kernel, sensible_feature=feature, C=C, power=4, coef=1, correlation=correlation)\n",
    "            clf.fit(X, y)\n",
    "            \n",
    "            # append to the plot data\n",
    "            w.append(clf.w)\n",
    "            b.append(clf.b)\n",
    "            xfit.append(clf.support_vectors)\n",
    "            f_feature.append(str(p_feature))\n",
    "            \n",
    "            \n",
    "            y_pred = clf.predict(X)\n",
    "\n",
    "            accuracy = accuracy_score(y, y_pred)\n",
    "            print (f'Accuracy for custom svm classifier, data_type = {data_type} constraints = {p_feature}: {accuracy}, c_value={correlation}')\n",
    "            \n",
    "            p_rule_string = \"p_rule on = \" + str(p_feature) + \" , \" + data_type\n",
    "            p_rule = compute_p_rule(p_rule_string, x_control, y_pred)   \n",
    "            \n",
    "            X_svm, y_svm, x_control_svm = X, y, x_control\n",
    "            if (kernel.__name__ == 'rbf_kernel'):\n",
    "                svc = svm.SVC(kernel='rbf', C=C).fit(X_svm, y_svm)\n",
    "            else:\n",
    "                svc = svm.SVC(kernel='linear', C=C).fit(X_svm, y_svm)\n",
    "            print(f'SVC kernel details {svc}')\n",
    "            y_pred_svm = svc.predict(X_svm)\n",
    "            accuracy_svm = accuracy_score(y_svm, y_pred_svm)\n",
    "            print (f'Accuracy(svm) for data_type = {data_type} constraints = {p_feature}: {accuracy_svm}')\n",
    "            \n",
    "            if (feature is not None):\n",
    "                compute_p_rule(\"p_rule on svm classifier on \" + data_type + \", constraints = \" + str(p_feature), x_control, y_pred_svm)\n",
    "              \n",
    "            results_data['samples(train)'].append(X.shape[0])\n",
    "            results_data['p_rule'].append(p_rule)\n",
    "            results_data['data_type'].append(data_type)\n",
    "            results_data['constraints'].append(p_feature)\n",
    "            results_data['accuracy'].append(accuracy)\n",
    "            results_data['accuracy_svm(no_constraints)'].append(accuracy_svm)\n",
    "            results_data['kernel'].append(kernel.__name__)\n",
    "            results_data['C'].append(C)\n",
    "            results_data['coeff_c'].append(correlation)\n",
    "                \n",
    "            \n",
    "            print('**'*50)\n",
    "    \n",
    "    return w,b,xfit,f_feature\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of synthetic X = (1000, 2), y=(1000,), x_sensitive=(1000,)\n",
      "P-rule is: 44%\n",
      "C=1,None\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1788e+02 -2.6550e+03  1e+04  3e+00  1e-13\n",
      " 1: -2.7801e+02 -1.7758e+03  3e+03  4e-01  1e-13\n",
      " 2: -2.3552e+02 -5.1627e+02  4e+02  3e-02  7e-14\n",
      " 3: -2.5930e+02 -3.3005e+02  8e+01  7e-03  5e-14\n",
      " 4: -2.7145e+02 -3.0669e+02  4e+01  3e-03  4e-14\n",
      " 5: -2.7625e+02 -2.9751e+02  2e+01  1e-03  5e-14\n",
      " 6: -2.7947e+02 -2.9194e+02  1e+01  7e-04  5e-14\n",
      " 7: -2.8079e+02 -2.8952e+02  9e+00  4e-04  5e-14\n",
      " 8: -2.8224e+02 -2.8719e+02  5e+00  2e-04  5e-14\n",
      " 9: -2.8392e+02 -2.8480e+02  9e-01  1e-15  6e-14\n",
      "10: -2.8431e+02 -2.8439e+02  8e-02  1e-14  6e-14\n",
      "11: -2.8434e+02 -2.8435e+02  7e-03  9e-15  6e-14\n",
      "12: -2.8435e+02 -2.8435e+02  2e-03  7e-16  6e-14\n",
      "13: -2.8435e+02 -2.8435e+02  1e-03  9e-15  6e-14\n",
      "14: -2.8435e+02 -2.8435e+02  1e-05  2e-14  6e-14\n",
      "15: -2.8435e+02 -2.8435e+02  1e-07  8e-15  6e-14\n",
      "16: -2.8435e+02 -2.8435e+02  1e-09  1e-14  6e-14\n",
      "Optimal solution found.\n",
      "Accuracy for custom svm classifier, data_type = synthetic constraints = None: 0.888, c_value=0.0\n",
      "P-rule is: 36%\n",
      "SVC kernel details SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "Accuracy(svm) for data_type = synthetic constraints = None: 0.888\n",
      "****************************************************************************************************\n",
      "C=1,Fairness Constraints\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1788e+02 -2.6550e+03  1e+04  3e+00  1e-13\n",
      " 1: -2.7804e+02 -1.7757e+03  3e+03  4e-01  1e-13\n",
      " 2: -2.3561e+02 -5.1631e+02  4e+02  3e-02  7e-14\n",
      " 3: -2.5925e+02 -3.2973e+02  8e+01  7e-03  5e-14\n",
      " 4: -2.7145e+02 -3.0629e+02  4e+01  3e-03  5e-14\n",
      " 5: -2.7606e+02 -2.9742e+02  2e+01  1e-03  5e-14\n",
      " 6: -2.7921e+02 -2.9200e+02  1e+01  7e-04  5e-14\n",
      " 7: -2.8077e+02 -2.8920e+02  9e+00  4e-04  5e-14\n",
      " 8: -2.8219e+02 -2.8690e+02  5e+00  1e-04  5e-14\n",
      " 9: -2.8359e+02 -2.8496e+02  1e+00  2e-05  6e-14\n",
      "10: -2.8416e+02 -2.8428e+02  1e-01  1e-06  6e-14\n",
      "11: -2.8421e+02 -2.8422e+02  1e-02  1e-07  6e-14\n",
      "12: -2.8421e+02 -2.8422e+02  1e-03  9e-09  6e-14\n",
      "13: -2.8421e+02 -2.8421e+02  1e-05  1e-10  7e-14\n",
      "14: -2.8421e+02 -2.8421e+02  1e-07  1e-12  6e-14\n",
      "15: -2.8421e+02 -2.8421e+02  1e-09  3e-14  7e-14\n",
      "Optimal solution found.\n",
      "Accuracy for custom svm classifier, data_type = synthetic constraints = Fairness Constraints: 0.799, c_value=0.0\n",
      "P-rule is: 69%\n",
      "SVC kernel details SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "Accuracy(svm) for data_type = synthetic constraints = Fairness Constraints: 0.888\n",
      "P-rule is: 36%\n",
      "****************************************************************************************************\n",
      "w=[[0.31917426501215007, 0.6874057087800907], [0.5138621686625162, 0.20920785918829465]],b=float64,feature=['None', 'Fairness Constraints']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'numpy.float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-810c9489cfa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Compare the statistics of the accuracies across all cross-validation folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxfit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_s\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_control_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinear_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'synthetic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxfit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#run_experiment(X_s, y_s , x_control_s, [1], kernel=linear_kernel, data_type='synthetic', correlation=0.1, results=results_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c2d455edb3fb>\u001b[0m in \u001b[0;36mplot_decision_boundary\u001b[0;34m(X, y, w, b, xfit, feature)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'w={w},b={b[0].dtype},feature={feature}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m#print(f'{w[0][0]}, {w[0][1]}, {w[1][0]}, {w[1][1]}, {b[0]}, {b[1]}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxfit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.float64'"
     ]
    }
   ],
   "source": [
    "data_count=500\n",
    "\n",
    "X_s, y_s , x_control_s = get_data('synthetic',sample_count=500)\n",
    "\n",
    "#X_a, y_a , x_control_a = get_data('adult', sample_count=10000)\n",
    "\n",
    "# Compare the statistics of the accuracies across all cross-validation folds\n",
    "w,b,xfit,feature = run_experiment(X_s, y_s , x_control_s, [1], kernel=linear_kernel, data_type='synthetic', results=results_data)\n",
    "plot_decision_boundary(X_s, y_s, w,b,xfit,feature)\n",
    "\n",
    "#run_experiment(X_s, y_s , x_control_s, [1], kernel=linear_kernel, data_type='synthetic', correlation=0.1, results=results_data)\n",
    "\n",
    "#run_experiment(X_s, y_s , x_control_s, [1], kernel=linear_kernel, data_type='synthetic', correlation=1, results=results_data)\n",
    "\n",
    "#run_experiment(X_a, y_a , x_control_a, [1], data_type='adult', correlation=0.01, results=results_data)\n",
    "#run_experiment(X_a, y_a , x_control_a, [1], data_type='adult', results=results_data)\n",
    "\n",
    "results_df = pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bcd1db947474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_data' is not defined"
     ]
    }
   ],
   "source": [
    "plot_decision_boundary(X_s, y_s, plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_b, y_b , x_control_b = get_data('bank', sample_count=5000)\n",
    "#run_experiment(X_b, y_b , x_control_b, [10], data_type='bank', results=results_data)\n",
    "#results_df = pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

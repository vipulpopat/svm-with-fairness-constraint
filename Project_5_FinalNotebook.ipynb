{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of topics for the final project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 algorithmic fairness\n",
    "Algorithm fairness is becoming a fundamental topic in ML. It is a complex ethical task to define what fairness is/means. Once we have defined quantitatively what fairness is then, from a mathematical perspective, the problem of  algorithmic fairness is very clear: it is a multi-objective optimization problem. There are multi objectives that we aim to optimize during data fitting, e.g., accuracy and fairness.\n",
    "\n",
    "The goal of this project is to implement from scratch the \"fair\" **linear and nonlinear SVM** described in the following paper (see in particular Appendix A that reports the optimization problem)  \n",
    "\n",
    "[\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf)\n",
    "\n",
    "and reproduce the experiments reported in the paper. In particular, apply your method to the Adult and Bank \n",
    "data sets.\n",
    "\n",
    "Your notebook must include:\n",
    "* a description (summary) of the algorithm presented in the above paper (focusing on SVM), similar to the theoretical details of logistic regression I wrote at the beginning of the notebook for e-tivity Task A (week 1&2). The reader must understand from your explanation the difference between standard SVM and the \"fair\" SVM.\n",
    "* You implementation of the \"fair\" **linear and nonlinear SVM** using CVXOPT to solve data fitting (as I have shown in Week 3 webinar, see also example below). You should implement it as a Python class (similar to logistic regression class for E-tivity 1).\n",
    "* A test of the input-output behavior of your algorithm. More clearly, you have to replicate the experiment results you find in Section 4.1 for Synthetic Data and Section 4.2 of the above paper for the Adult and Bank data sets.\n",
    "\n",
    "\n",
    "Resources:\n",
    "* Week 3 webinar slides with the details of the SVM algorithm;\n",
    "* [Example](https://xavierbourretsicotte.github.io/SVM_implementation.html) about how to use the library CVXOPT to implement data fitting for standard SVM\n",
    "* [fairness-in-machine-learning](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)\n",
    "* (Optional) Multi-objective optimization and Pareto optimality see Book chapter 12 (of our Module's book).\n",
    "\n",
    "**How to approach the problem** (this is just a suggestion).\n",
    "\n",
    "You can start implementing linear SVM and apply it to the Synthetic Data experiment in Section 4.1 so that you can plot the classification line for standard linear SVM versus fair linear SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:45.225887Z",
     "start_time": "2020-02-15T12:53:29.993350Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import svm\n",
    "import cvxopt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel_cust(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def linear_kernel(**kwargs):\n",
    "    def f(x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "    return f\n",
    "\n",
    "\n",
    "def polynomial_kernel(power, coef, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        return (np.dot(x1, x2) + coef)**power\n",
    "    return f\n",
    "\n",
    "\n",
    "def rbf_kernel(gamma, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        distance = np.linalg.norm(x1 - x2) ** 2\n",
    "        return np.exp(-gamma * distance)\n",
    "    return f\n",
    "\n",
    "def compute_p_rule(print_string, x_control, class_labels):\n",
    "\n",
    "    \"\"\" Compute the p-rule based on Doctrine of disparate impact \"\"\"\n",
    "\n",
    "    non_prot_all = sum(x_control == 1.0) # non-protected group\n",
    "    prot_all = sum(x_control == 0.0) # protected group\n",
    "    non_prot_pos = sum(class_labels[x_control == 1.0] == 1.0) # non_protected in positive class\n",
    "    prot_pos = sum(class_labels[x_control == 0.0] == 1.0) # protected in positive class\n",
    "    frac_non_prot_pos = float(non_prot_pos) / float(non_prot_all)\n",
    "    frac_prot_pos = float(prot_pos) / float(prot_all)\n",
    "    p_rule = (frac_prot_pos / frac_non_prot_pos) * 100.0\n",
    "    print ()\n",
    "    print((\"Total data points: %d\" % (len(x_control))))\n",
    "    print((\"# non-protected examples: %d\" % (non_prot_all)))\n",
    "    print((\"# protected examples: %d\" % (prot_all)))\n",
    "    print((\"Non-protected in positive class: %d (%0.0f%%)\" % (non_prot_pos, non_prot_pos * 100.0 / non_prot_all)))\n",
    "    print((\"Protected in positive class: %d (%0.0f%%)\" % (prot_pos, prot_pos * 100.0 / prot_all)))\n",
    "    print((\"P-rule is: %0.0f%%\" % ( p_rule )))\n",
    "    print(f'{print_string} = {p_rule}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:48.097038Z",
     "start_time": "2020-02-15T12:53:47.888157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hide cvxopt output\n",
    "cvxopt.solvers.options['show_progress'] = True\n",
    "\n",
    "class SupportVectorMachine(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=None, kernel=rbf_kernel, sensible_feature=None, power=4, gamma=None, coef=4, correlation=0.0):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature   \n",
    "        self.correlation = correlation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        if self.fairness:\n",
    "            self.values_of_sensible_feature = list(set(self.sensible_feature))\n",
    "            self.list_of_sensible_feature_train = self.sensible_feature\n",
    "            self.val0 = np.min(self.values_of_sensible_feature)\n",
    "            self.val1 = np.max(self.values_of_sensible_feature)\n",
    "            self.set_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                           and self.sensible_feature[idx] == self.val1]\n",
    "            self.set_not_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                               and self.sensible_feature[idx] == self.val0]\n",
    "            self.set_1 = [idx for idx, ex in enumerate(X) if y[idx] == 1]\n",
    "            self.n_A1 = len(self.set_A1)\n",
    "            self.n_not_A1 = len(self.set_not_A1)\n",
    "            self.n_1 = len(self.set_1)  \n",
    "            \n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if not self.C:\n",
    "            G = cvxopt.matrix(np.identity(n_samples) * -1)\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            G_max = np.identity(n_samples) * -1\n",
    "            G_min = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n",
    "            h_max = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n",
    "            \n",
    "        # Stack the fairness constraint\n",
    "        if self.fairness:\n",
    "            tau = [(np.sum(kernel_matrix[self.set_A1, idx]) / self.n_A1) - (np.sum(kernel_matrix[self.set_not_A1, idx]) / self.n_not_A1) for idx in range(len(y))]\n",
    "            fairness_line = cvxopt.matrix(y * tau, (1, n_samples), 'd')\n",
    "            A = cvxopt.matrix(np.vstack([A, fairness_line]))\n",
    "            b = cvxopt.matrix([0.0, self.correlation])            \n",
    "            \n",
    "        #print(\"P.shape\", P.size)\n",
    "        #print(\"q.shape\", q.size)\n",
    "        #print(\"G.shape\", G.size)\n",
    "        #print(\"h.shape\", h.size)\n",
    "        #print(\"A.shape\", A.size)\n",
    "        #print(\"b.shape\", b.size)\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "\n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > 1e-7\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorMachine_trial(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=None, kernel=rbf_kernel, sensible_feature=None, power=4, gamma=None, coef=4):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        if self.fairness:\n",
    "            self.values_of_sensible_feature = list(set(self.sensible_feature))\n",
    "            self.list_of_sensible_feature_train = self.sensible_feature\n",
    "            self.val0 = np.min(self.values_of_sensible_feature)\n",
    "            self.val1 = np.max(self.values_of_sensible_feature)\n",
    "            self.set_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                           and self.sensible_feature[idx] == self.val1]\n",
    "            self.set_not_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                               and self.sensible_feature[idx] == self.val0]\n",
    "            self.set_1 = [idx for idx, ex in enumerate(X) if y[idx] == 1]\n",
    "            self.n_A1 = len(self.set_A1)\n",
    "            self.n_not_A1 = len(self.set_not_A1)\n",
    "            self.n_1 = len(self.set_1)  \n",
    "         \n",
    "        # Stack the fairness constraint\n",
    "        if self.fairness:\n",
    "            tau = [(np.sum(kernel_matrix[self.set_A1, idx]) / self.n_A1) - (np.sum(kernel_matrix[self.set_not_A1, idx]) / self.n_not_A1) for idx in range(len(y))]\n",
    "            print(f\"Tauji kee value {tau[0:10]}\")\n",
    "            fairness_line = cvxopt.matrix(y * tau, (1, n_samples), 'd')\n",
    "            print(\"Fairness_line\", fairness_line)\n",
    "        \n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1, tc='d')\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if self.C:\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1,np.identity(n_samples))), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)), tc='d')\n",
    "        \n",
    "        elif self.C and self.fairness:\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1,np.identity(n_samples)), np.identity(n_samples) * tau), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack(np.zeros(n_samples), np.ones(n_samples) * C, np.ones(n_samples) * 0.1), tc='d')\n",
    "        else:\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1)), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack((np.zeros(n_samples))), tc='d')\n",
    "\n",
    "        print(\"P.shape\", P.size)\n",
    "        print(\"q.shape\", q.size)\n",
    "        print(\"G.shape\", G.size)\n",
    "        print(\"h.shape\", h.size)\n",
    "        print(\"A.shape\", A.size)\n",
    "        print(\"b.shape\", b.size)\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "\n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > 1e-7\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0, './fair-classification3/fair_classification') # the code for fair classification is in this directory\n",
    "\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import numpy as np\n",
    "from random import seed, shuffle\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "SEED = 1122\n",
    "seed(SEED) # set the random seed so that the random permutations can be reproduced again\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the adult data        \n",
    "def load_adult_data(load_data_size=None, drop_sensitive=True):\n",
    "\n",
    "   # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    attrs = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country','target'] \n",
    "    \n",
    "    data = pd.read_csv(\"dataset/adult/adult.data\", names=attrs, skipinitialspace=True) \n",
    "\n",
    "    data = data.replace(to_replace ='?', value =np.NaN)\n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    target_mapping = {'<=50K': -1, '>50K': 1,'<=50K.': -1, '>50K.': 1}\n",
    "    data.replace({\"target\": target_mapping}, inplace=True)\n",
    "    data['native_country'] = data['native_country'].apply(lambda x: 'USA' if x in ['United-States'] else 'NON-USA')\n",
    "    data['education'] = data['education'].apply(lambda x: 'pre-middle-school' if x in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"] else 'high-school')\n",
    "\n",
    "    encoded_object_df = pd.DataFrame()\n",
    "\n",
    "    for column in ['workclass', 'sex', 'education', 'marital_status', 'occupation','relationship',  'native_country']:\n",
    "        # race\n",
    "        encoded_object_df = pd.concat([encoded_object_df,pd.get_dummies(data[column], prefix=column, drop_first=True)] ,axis=1)\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    cols_to_scale = ['age', 'education_num', 'capital_gain', 'capital_loss', 'capital_loss', 'target']\n",
    "    #fnlwgt\n",
    "\n",
    "    encoded_int_df = data[cols_to_scale]\n",
    "\n",
    "    encoded_int_df[cols_to_scale] = min_max_scaler.fit_transform(encoded_int_df[cols_to_scale])\n",
    "    \n",
    "    final_df = pd.concat([encoded_object_df, encoded_int_df], axis=1)\n",
    "    #final_df = final_df.sample(n=load_data_size)\n",
    "    final_df = final_df.iloc[0:load_data_size]\n",
    "    \n",
    "    # shuffle the data\n",
    "    \n",
    "    y = np.where(final_df['target'] == 0, -1, 1)\n",
    "    x_sensitive = np.array(final_df['sex_Male'])\n",
    "    \n",
    "    if drop_sensitive:\n",
    "        final_df.drop(['sex_Male'], axis=1, inplace=True)\n",
    "        \n",
    "    final_df.drop(['target'], axis=1, inplace=True)\n",
    "        \n",
    "    X = np.array(final_df)\n",
    "    \n",
    "    print(f\"Shapes of X = {X.shape}, y={y.shape}, x_sensitive={x_sensitive.shape}\")\n",
    "    \n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load bank data\n",
    "def load_bank_data(load_data_size=None, drop_sensitive=True):\n",
    "\n",
    "   # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    attrs = [\"age\",\"job\",\"marital\",\"education\",\"default\",\"balance\",\"housing\",\"loan\",\"contact\",\"day\",\"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\",\"y\"] \n",
    "    \n",
    "    data = pd.read_csv(\"dataset/bank/bank.csv\", skipinitialspace=True, delimiter=';') \n",
    "\n",
    "    data['month-seasonal'] = data['month'].apply(lambda x: 'q1' if x in [\"jan\", \"feb\", \"mar\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q2' if x in [\"apr\", \"may\", \"jun\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q3' if x in [\"jul\", \"aug\", \"sep\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q4' if x in [\"oct\", \"nov\", \"dec\"] else x )\n",
    "    data['age-range'] = data['age'].apply(lambda x: 0.0 if (x >=25 and x<=60)  else 1.0 )\n",
    "\n",
    "    data.drop(['month'], inplace=True, axis=1)\n",
    "    \n",
    "    encoded_object_df = pd.DataFrame()\n",
    "\n",
    "    for column in ['job', 'marital', 'education', 'default', 'housing','loan','contact', 'poutcome','y','month-seasonal']:\n",
    "        encoded_object_df = pd.concat([encoded_object_df,pd.get_dummies(data[column], prefix=column, drop_first=True)] ,axis=1)\n",
    "    \n",
    "    min_max_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    cols_to_scale = ['balance','day','duration', 'campaign', 'pdays','previous']\n",
    "\n",
    "    encoded_int_df = data[cols_to_scale]\n",
    "\n",
    "    encoded_int_df[cols_to_scale] = min_max_scaler.fit_transform(encoded_int_df[cols_to_scale])\n",
    "    \n",
    "    final_df = pd.concat([encoded_object_df, encoded_int_df, data['age-range']], axis=1)\n",
    "    \n",
    "    final_df = final_df.sample(n=load_data_size, random_state=1234)\n",
    "    \n",
    "    y = np.where(final_df['y_yes'] == 0, -1, 1)\n",
    "    x_sensitive = np.array(final_df['age-range'])\n",
    "\n",
    "    \n",
    "    if drop_sensitive:\n",
    "        final_df.drop(['age-range'], axis=1, inplace=True)\n",
    "        \n",
    "    final_df.drop(['y_yes'], axis=1, inplace=True)\n",
    "        \n",
    "    X = np.array(final_df)\n",
    "    \n",
    "    print(f\"Shapes of X = {X.shape}, y={y.shape}, x_sensitive={x_sensitive.shape}\")\n",
    "    \n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(C_values, sample_count, data_type, correlation=0.0):\n",
    "    if data_type == 'bank':\n",
    "        X, y, x_control = load_bank_data(load_data_size=sample_count)\n",
    "    else:\n",
    "        X, y, x_control = load_adult_data(load_data_size=sample_count)\n",
    "        \n",
    "    ## compute the p_rule on base scenario\n",
    "    compute_p_rule(\"Base Data \" + data_type, x_control, y)\n",
    "    \n",
    "    for C in C_values:\n",
    "        for feature in [None, x_control]:\n",
    "            p_feature = None\n",
    "            if (feature is not None):\n",
    "                p_feature = 'With Fairness Constraints'\n",
    "            print(f'C={C},{p_feature}')\n",
    "            clf = SupportVectorMachine(kernel=rbf_kernel, sensible_feature=feature, C=C, power=4, coef=1, correlation=correlation)\n",
    "            #clf = SupportVectorMachine_trial(kernel=rbf_kernel, sensible_feature=feature, C=C, power=4, coef=1)\n",
    "            clf.fit(X, y)\n",
    "            y_pred = clf.predict(X)\n",
    "\n",
    "            accuracy = accuracy_score(y, y_pred)\n",
    "            print (\"Accuracy:\", accuracy)\n",
    "            p_rule_string = \"p_rule on = \" + str(p_feature) + \" , \" + data_type\n",
    "            compute_p_rule(p_rule_string, x_control, y_pred)   \n",
    "            \n",
    "            X_svm, y_svm, x_control_svm = X, y, x_control\n",
    "            svc = svm.SVC(kernel='rbf', C=1).fit(X_svm, y_svm)\n",
    "\n",
    "            y_pred_svm = svc.predict(X_svm)\n",
    "            accuracy_svm = accuracy_score(y_svm, y_pred_svm)\n",
    "            print (\"Accuracy of SVM Classifier:\", accuracy_svm)\n",
    "            \n",
    "            compute_p_rule(\"p_rule on svm classifier on \" + data_type, x_control, y_pred_svm)\n",
    "            print('**'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of X = (2000, 33), y=(2000,), x_sensitive=(2000,)\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 93\n",
      "# protected examples: 1907\n",
      "Non-protected in positive class: 27 (29%)\n",
      "Protected in positive class: 191 (10%)\n",
      "P-rule is: 34%\n",
      "Base Data bank = 34.49863077550545\n",
      "C=10,None\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.0994e+03 -7.3513e+04  1e+05  3e-01  6e-14\n",
      " 1: -1.9209e+03 -1.8422e+04  2e+04  3e-02  6e-14\n",
      " 2: -2.5306e+03 -8.2709e+03  6e+03  8e-03  6e-14\n",
      " 3: -2.9212e+03 -6.1572e+03  3e+03  3e-03  7e-14\n",
      " 4: -3.1298e+03 -5.0945e+03  2e+03  2e-03  7e-14\n",
      " 5: -3.2635e+03 -4.4735e+03  1e+03  9e-04  8e-14\n",
      " 6: -3.3833e+03 -3.9462e+03  6e+02  2e-04  8e-14\n",
      " 7: -3.4318e+03 -3.7753e+03  3e+02  1e-04  8e-14\n",
      " 8: -3.4727e+03 -3.6332e+03  2e+02  3e-13  1e-13\n",
      " 9: -3.4973e+03 -3.5678e+03  7e+01  2e-13  9e-14\n",
      "10: -3.5062e+03 -3.5470e+03  4e+01  2e-13  1e-13\n",
      "11: -3.5140e+03 -3.5304e+03  2e+01  2e-13  1e-13\n",
      "12: -3.5150e+03 -3.5286e+03  1e+01  1e-13  9e-14\n",
      "13: -3.5190e+03 -3.5220e+03  3e+00  4e-13  1e-13\n",
      "14: -3.5199e+03 -3.5207e+03  8e-01  3e-13  1e-13\n",
      "15: -3.5202e+03 -3.5202e+03  3e-02  7e-14  1e-13\n",
      "16: -3.5202e+03 -3.5202e+03  7e-04  5e-14  1e-13\n",
      "Optimal solution found.\n",
      "Accuracy: 0.937\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 93\n",
      "# protected examples: 1907\n",
      "Non-protected in positive class: 21 (23%)\n",
      "Protected in positive class: 145 (8%)\n",
      "P-rule is: 34%\n",
      "p_rule on = None , bank = 33.67293430219492\n",
      "Accuracy of SVM Classifier: 0.9165\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 93\n",
      "# protected examples: 1907\n",
      "Non-protected in positive class: 5 (5%)\n",
      "Protected in positive class: 60 (3%)\n",
      "P-rule is: 59%\n",
      "p_rule on svm classifier on bank = 58.52123754588357\n",
      "****************************************************************************************************\n",
      "C=10,With Fairness Constraints\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.1040e+03 -7.3361e+04  1e+05  3e-01  6e-14\n",
      " 1: -1.9241e+03 -1.8803e+04  2e+04  3e-02  6e-14\n",
      " 2: -2.5204e+03 -8.3813e+03  6e+03  8e-03  6e-14\n",
      " 3: -2.9241e+03 -6.1434e+03  3e+03  3e-03  7e-14\n",
      " 4: -3.1294e+03 -5.1039e+03  2e+03  2e-03  7e-14\n",
      " 5: -3.2615e+03 -4.4863e+03  1e+03  9e-04  7e-14\n",
      " 6: -3.3817e+03 -3.9518e+03  6e+02  2e-04  9e-14\n",
      " 7: -3.4309e+03 -3.7771e+03  3e+02  1e-04  8e-14\n",
      " 8: -3.4726e+03 -3.6320e+03  2e+02  4e-13  1e-13\n",
      " 9: -3.4971e+03 -3.5672e+03  7e+01  1e-13  9e-14\n",
      "10: -3.5059e+03 -3.5464e+03  4e+01  2e-13  1e-13\n",
      "11: -3.5137e+03 -3.5298e+03  2e+01  5e-13  1e-13\n",
      "12: -3.5147e+03 -3.5281e+03  1e+01  5e-13  9e-14\n",
      "13: -3.5186e+03 -3.5217e+03  3e+00  6e-13  1e-13\n",
      "14: -3.5195e+03 -3.5204e+03  9e-01  5e-13  1e-13\n",
      "15: -3.5198e+03 -3.5199e+03  4e-02  6e-14  1e-13\n",
      "16: -3.5199e+03 -3.5199e+03  7e-04  2e-13  1e-13\n",
      "Optimal solution found.\n",
      "Accuracy: 0.94\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 93\n",
      "# protected examples: 1907\n",
      "Non-protected in positive class: 27 (29%)\n",
      "Protected in positive class: 141 (7%)\n",
      "P-rule is: 25%\n",
      "p_rule on = With Fairness Constraints , bank = 25.46757559867156\n",
      "Accuracy of SVM Classifier: 0.9165\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 93\n",
      "# protected examples: 1907\n",
      "Non-protected in positive class: 5 (5%)\n",
      "Protected in positive class: 60 (3%)\n",
      "P-rule is: 59%\n",
      "p_rule on svm classifier on bank = 58.52123754588357\n",
      "****************************************************************************************************\n",
      "Shapes of X = (2000, 37), y=(2000,), x_sensitive=(2000,)\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 1373\n",
      "# protected examples: 627\n",
      "Non-protected in positive class: 427 (31%)\n",
      "Protected in positive class: 87 (14%)\n",
      "P-rule is: 45%\n",
      "Base Data adult = 44.61638447833444\n",
      "C=1,None\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.8266e+02 -5.3304e+03  3e+04  3e+00  2e-14\n",
      " 1: -6.6197e+02 -3.5473e+03  4e+03  2e-01  2e-14\n",
      " 2: -7.0830e+02 -1.1924e+03  5e+02  2e-02  1e-14\n",
      " 3: -7.9469e+02 -9.7105e+02  2e+02  6e-03  1e-14\n",
      " 4: -8.2385e+02 -9.1260e+02  9e+01  2e-03  1e-14\n",
      " 5: -8.3924e+02 -8.8427e+02  5e+01  9e-04  1e-14\n",
      " 6: -8.4789e+02 -8.6940e+02  2e+01  4e-04  1e-14\n",
      " 7: -8.5286e+02 -8.6124e+02  8e+00  9e-05  2e-14\n",
      " 8: -8.5497e+02 -8.5821e+02  3e+00  2e-05  2e-14\n",
      " 9: -8.5603e+02 -8.5687e+02  8e-01  3e-06  2e-14\n",
      "10: -8.5633e+02 -8.5652e+02  2e-01  7e-07  2e-14\n",
      "11: -8.5641e+02 -8.5643e+02  2e-02  7e-08  2e-14\n",
      "12: -8.5642e+02 -8.5642e+02  8e-04  3e-10  2e-14\n",
      "Optimal solution found.\n",
      "Accuracy: 0.7955\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 1373\n",
      "# protected examples: 627\n",
      "Non-protected in positive class: 582 (42%)\n",
      "Protected in positive class: 149 (24%)\n",
      "P-rule is: 56%\n",
      "p_rule on = None , adult = 56.06170220928767\n",
      "Accuracy of SVM Classifier: 0.8315\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 1373\n",
      "# protected examples: 627\n",
      "Non-protected in positive class: 335 (24%)\n",
      "Protected in positive class: 52 (8%)\n",
      "P-rule is: 34%\n",
      "p_rule on svm classifier on adult = 33.99081149277536\n",
      "****************************************************************************************************\n",
      "C=1,With Fairness Constraints\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.8279e+02 -5.3310e+03  3e+04  3e+00  2e-14\n",
      " 1: -6.6276e+02 -3.5473e+03  4e+03  2e-01  2e-14\n",
      " 2: -7.0841e+02 -1.1971e+03  5e+02  2e-02  1e-14\n",
      " 3: -7.9442e+02 -9.7169e+02  2e+02  6e-03  1e-14\n",
      " 4: -8.2298e+02 -9.1356e+02  9e+01  2e-03  1e-14\n",
      " 5: -8.3877e+02 -8.8419e+02  5e+01  9e-04  1e-14\n",
      " 6: -8.4724e+02 -8.6956e+02  2e+01  4e-04  1e-14\n",
      " 7: -8.5231e+02 -8.6125e+02  9e+00  1e-04  2e-14\n",
      " 8: -8.5452e+02 -8.5799e+02  4e+00  3e-05  2e-14\n",
      " 9: -8.5567e+02 -8.5648e+02  8e-01  4e-06  2e-14\n",
      "10: -8.5596e+02 -8.5613e+02  2e-01  7e-07  2e-14\n",
      "11: -8.5603e+02 -8.5605e+02  2e-02  5e-08  2e-14\n",
      "12: -8.5604e+02 -8.5604e+02  3e-04  9e-10  2e-14\n",
      "Optimal solution found.\n",
      "Accuracy: 0.744\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 1373\n",
      "# protected examples: 627\n",
      "Non-protected in positive class: 1 (0%)\n",
      "Protected in positive class: 1 (0%)\n",
      "P-rule is: 219%\n",
      "p_rule on = With Fairness Constraints , adult = 218.9792663476874\n",
      "Accuracy of SVM Classifier: 0.8315\n",
      "\n",
      "Total data points: 2000\n",
      "# non-protected examples: 1373\n",
      "# protected examples: 627\n",
      "Non-protected in positive class: 335 (24%)\n",
      "Protected in positive class: 52 (8%)\n",
      "P-rule is: 34%\n",
      "p_rule on svm classifier on adult = 33.99081149277536\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "data_count=2000\n",
    "run_experiment([10], sample_count=data_count, data_type='bank')\n",
    "run_experiment([1], sample_count=data_count, data_type='adult', correlation=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of topics for the final project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5 algorithmic fairness\n",
    "Algorithm fairness is becoming a fundamental topic in ML. It is a complex ethical task to define what fairness is/means. Once we have defined quantitatively what fairness is then, from a mathematical perspective, the problem of  algorithmic fairness is very clear: it is a multi-objective optimization problem. There are multi objectives that we aim to optimize during data fitting, e.g., accuracy and fairness.\n",
    "\n",
    "The goal of this project is to implement from scratch the \"fair\" **linear and nonlinear SVM** described in the following paper (see in particular Appendix A that reports the optimization problem)  \n",
    "\n",
    "[\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf)\n",
    "\n",
    "and reproduce the experiments reported in the paper. In particular, apply your method to the Adult and Bank \n",
    "data sets.\n",
    "\n",
    "Your notebook must include:\n",
    "* a description (summary) of the algorithm presented in the above paper (focusing on SVM), similar to the theoretical details of logistic regression I wrote at the beginning of the notebook for e-tivity Task A (week 1&2). The reader must understand from your explanation the difference between standard SVM and the \"fair\" SVM.\n",
    "* You implementation of the \"fair\" **linear and nonlinear SVM** using CVXOPT to solve data fitting (as I have shown in Week 3 webinar, see also example below). You should implement it as a Python class (similar to logistic regression class for E-tivity 1).\n",
    "* A test of the input-output behavior of your algorithm. More clearly, you have to replicate the experiment results you find in Section 4.1 for Synthetic Data and Section 4.2 of the above paper for the Adult and Bank data sets.\n",
    "\n",
    "\n",
    "Resources:\n",
    "* Week 3 webinar slides with the details of the SVM algorithm;\n",
    "* [Example](https://xavierbourretsicotte.github.io/SVM_implementation.html) about how to use the library CVXOPT to implement data fitting for standard SVM\n",
    "* [fairness-in-machine-learning](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)\n",
    "* (Optional) Multi-objective optimization and Pareto optimality see Book chapter 12 (of our Module's book).\n",
    "\n",
    "**How to approach the problem** (this is just a suggestion).\n",
    "\n",
    "You can start implementing linear SVM and apply it to the Synthetic Data experiment in Section 4.1 so that you can plot the classification line for standard linear SVM versus fair linear SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:45.225887Z",
     "start_time": "2020-02-15T12:53:29.993350Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import svm\n",
    "import cvxopt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## for synthetic data\n",
    "import math\n",
    "import matplotlib.pyplot as plt # for plotting stuff\n",
    "from random import seed, shuffle\n",
    "from scipy.stats import multivariate_normal # generating synthetic data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel_cust(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def linear_kernel(**kwargs):\n",
    "    def f(x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "    return f\n",
    "\n",
    "\n",
    "def polynomial_kernel(power, coef, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        return (np.dot(x1, x2) + coef)**power\n",
    "    return f\n",
    "\n",
    "\n",
    "def rbf_kernel(gamma, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        distance = np.linalg.norm(x1 - x2) ** 2\n",
    "        return np.exp(-gamma * distance)\n",
    "    return f\n",
    "\n",
    "def compute_p_rule(print_string, x_control, class_labels):\n",
    "\n",
    "    \"\"\" Compute the p-rule based on Doctrine of disparate impact \"\"\"\n",
    "    print(\"x_control=\", x_control)\n",
    "    print(\"class_labels=\", class_labels)\n",
    "    non_prot_all = sum(x_control == 1.0) # non-protected group\n",
    "    prot_all = sum(x_control == 0.0) # protected group\n",
    "    non_prot_pos = sum(class_labels[x_control == 1.0] == 1.0) # non_protected in positive class\n",
    "    prot_pos = sum(class_labels[x_control == 0.0] == 1.0) # protected in positive class\n",
    "    frac_non_prot_pos = float(non_prot_pos) / float(non_prot_all)\n",
    "    frac_prot_pos = float(prot_pos) / float(prot_all)\n",
    "    p_rule = (frac_prot_pos / frac_non_prot_pos) * 100.0\n",
    "    print ()\n",
    "    print((\"Total data points: %d\" % (len(x_control))))\n",
    "    print((\"# non-protected examples: %d\" % (non_prot_all)))\n",
    "    print((\"# protected examples: %d\" % (prot_all)))\n",
    "    print((\"Non-protected in positive class: %d (%0.0f%%)\" % (non_prot_pos, non_prot_pos * 100.0 / non_prot_all)))\n",
    "    print((\"Protected in positive class: %d (%0.0f%%)\" % (prot_pos, prot_pos * 100.0 / prot_all)))\n",
    "    print((\"P-rule is: %0.0f%%\" % ( p_rule )))\n",
    "    print(f'{print_string} = {p_rule}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T12:53:48.097038Z",
     "start_time": "2020-02-15T12:53:47.888157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hide cvxopt output\n",
    "cvxopt.solvers.options['show_progress'] = True\n",
    "\n",
    "class SupportVectorMachine(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=None, kernel=rbf_kernel, sensible_feature=None, power=4, gamma=None, coef=4, correlation=0.0):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature   \n",
    "        self.correlation = correlation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        if self.fairness:\n",
    "            self.values_of_sensible_feature = list(set(self.sensible_feature))\n",
    "            self.list_of_sensible_feature_train = self.sensible_feature\n",
    "            self.val0 = np.min(self.values_of_sensible_feature)\n",
    "            self.val1 = np.max(self.values_of_sensible_feature)\n",
    "            self.set_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                           and self.sensible_feature[idx] == self.val1]\n",
    "            self.set_not_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                               and self.sensible_feature[idx] == self.val0]\n",
    "            self.set_1 = [idx for idx, ex in enumerate(X) if y[idx] == 1]\n",
    "            self.n_A1 = len(self.set_A1)\n",
    "            self.n_not_A1 = len(self.set_not_A1)\n",
    "            self.n_1 = len(self.set_1)  \n",
    "            \n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if not self.C:\n",
    "            G = cvxopt.matrix(np.identity(n_samples) * -1)\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            G_max = np.identity(n_samples) * -1\n",
    "            G_min = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((G_max, G_min)))\n",
    "            h_max = cvxopt.matrix(np.zeros(n_samples))\n",
    "            h_min = cvxopt.matrix(np.ones(n_samples) * self.C)\n",
    "            h = cvxopt.matrix(np.vstack((h_max, h_min)))\n",
    "            \n",
    "        # Stack the fairness constraint\n",
    "        if self.fairness:\n",
    "            tau = [(np.sum(kernel_matrix[self.set_A1, idx]) / self.n_A1) - (np.sum(kernel_matrix[self.set_not_A1, idx]) / self.n_not_A1) for idx in range(len(y))]\n",
    "            fairness_line = cvxopt.matrix(y * tau, (1, n_samples), 'd')\n",
    "            A = cvxopt.matrix(np.vstack([A, fairness_line]))\n",
    "            b = cvxopt.matrix([0.0, self.correlation])            \n",
    "            \n",
    "        #print(\"P.shape\", P.size)\n",
    "        #print(\"q.shape\", q.size)\n",
    "        #print(\"G.shape\", G.size)\n",
    "        #print(\"h.shape\", h.size)\n",
    "        #print(\"A.shape\", A.size)\n",
    "        #print(\"b.shape\", b.size)\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "\n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > 1e-7\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorMachine_trial(object):\n",
    "    \"\"\"The Support Vector Machine classifier.\n",
    "    Uses cvxopt to solve the quadratic optimization problem.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C: float\n",
    "        Penalty term.\n",
    "    kernel: function\n",
    "        Kernel function. Can be either polynomial, rbf or linear.\n",
    "    power: int\n",
    "        The degree of the polynomial kernel. Will be ignored by the other\n",
    "        kernel functions.\n",
    "    gamma: float\n",
    "        Used in the rbf kernel function.\n",
    "    coef: float\n",
    "        Bias term used in the polynomial kernel function.\n",
    "    \"\"\"\n",
    "    def __init__(self, C=None, kernel=rbf_kernel, sensible_feature=None, power=4, gamma=None, coef=4):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.power = power\n",
    "        self.gamma = gamma\n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        self.fairness = False if sensible_feature is None else True\n",
    "        self.sensible_feature = sensible_feature        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Set gamma to 1/n_features by default\n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "\n",
    "        # Initialize kernel method with parameters\n",
    "        self.kernel = self.kernel(\n",
    "            power=self.power,\n",
    "            gamma=self.gamma,\n",
    "            coef=self.coef)\n",
    "\n",
    "        # Calculate kernel matrix\n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        if self.fairness:\n",
    "            self.values_of_sensible_feature = list(set(self.sensible_feature))\n",
    "            self.list_of_sensible_feature_train = self.sensible_feature\n",
    "            self.val0 = np.min(self.values_of_sensible_feature)\n",
    "            self.val1 = np.max(self.values_of_sensible_feature)\n",
    "            self.set_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                           and self.sensible_feature[idx] == self.val1]\n",
    "            self.set_not_A1 = [idx for idx, ex in enumerate(X) if y[idx] == 1\n",
    "                               and self.sensible_feature[idx] == self.val0]\n",
    "            self.set_1 = [idx for idx, ex in enumerate(X) if y[idx] == 1]\n",
    "            self.n_A1 = len(self.set_A1)\n",
    "            self.n_not_A1 = len(self.set_not_A1)\n",
    "            self.n_1 = len(self.set_1)  \n",
    "         \n",
    "        # Stack the fairness constraint\n",
    "        if self.fairness:\n",
    "            tau = [(np.sum(kernel_matrix[self.set_A1, idx]) / self.n_A1) - (np.sum(kernel_matrix[self.set_not_A1, idx]) / self.n_not_A1) for idx in range(len(y))]\n",
    "            print(f\"Tauji kee value {tau[0:10]}\")\n",
    "            fairness_line = cvxopt.matrix(y * tau, (1, n_samples), 'd')\n",
    "            print(\"Fairness_line\", fairness_line)\n",
    "        \n",
    "        # Define the quadratic optimization problem\n",
    "        P = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1, tc='d')\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc='d')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "\n",
    "        if self.C:\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1,np.identity(n_samples))), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)), tc='d')\n",
    "        \n",
    "        elif self.C and self.fairness:\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1,np.identity(n_samples)), np.identity(n_samples) * tau), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack(np.zeros(n_samples), np.ones(n_samples) * C, np.ones(n_samples) * 0.1), tc='d')\n",
    "        else:\n",
    "            G = cvxopt.matrix(np.vstack((np.identity(n_samples) * -1)), tc='d')\n",
    "            h = cvxopt.matrix(np.hstack((np.zeros(n_samples))), tc='d')\n",
    "\n",
    "        print(\"P.shape\", P.size)\n",
    "        print(\"q.shape\", q.size)\n",
    "        print(\"G.shape\", G.size)\n",
    "        print(\"h.shape\", h.size)\n",
    "        print(\"A.shape\", A.size)\n",
    "        print(\"b.shape\", b.size)\n",
    "\n",
    "        # Solve the quadratic optimization problem using cvxopt\n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "\n",
    "        # Extract support vectors\n",
    "        # Get indexes of non-zero lagr. multipiers\n",
    "        idx = lagr_mult > 1e-7\n",
    "        # Get the corresponding lagr. multipliers\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        # Get the samples that will act as support vectors\n",
    "        self.support_vectors = X[idx]\n",
    "        # Get the corresponding labels\n",
    "        self.support_vector_labels = y[idx]\n",
    "\n",
    "        # Calculate intercept with first support vector\n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                i] * self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        # Iterate through list of samples and make predictions\n",
    "        for sample in X:\n",
    "            prediction = 0\n",
    "            # Determine the label of the sample by the support vectors\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[\n",
    "                    i] * self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0, './fair-classification3/fair_classification') # the code for fair classification is in this directory\n",
    "\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import numpy as np\n",
    "from random import seed, shuffle\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "SEED = 1122\n",
    "seed(SEED) # set the random seed so that the random permutations can be reproduced again\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic data\n",
    "def load_synthetic_data(load_data_size=None, drop_sensitive=True, plot_data=False):\n",
    "\n",
    "    \"\"\"\n",
    "        Code for generating the synthetic data.\n",
    "        We will have two non-sensitive features and one sensitive feature.\n",
    "        A sensitive feature value of 0.0 means the example is considered to be in protected group (e.g., female) and 1.0 means it's in non-protected group (e.g., male).\n",
    "    \"\"\"\n",
    "\n",
    "    if (load_data_size == None):\n",
    "        n_samples = 1000 # generate these many data points per class\n",
    "    else:\n",
    "        n_samples = load_data_size\n",
    "        \n",
    "    disc_factor = math.pi / 4.0 # this variable determines the initial discrimination in the data -- decraese it to generate more discrimination\n",
    "\n",
    "    def gen_gaussian(mean_in, cov_in, class_label):\n",
    "        nv = multivariate_normal(mean = mean_in, cov = cov_in)\n",
    "        X = nv.rvs(n_samples)\n",
    "        y = np.ones(n_samples, dtype=float) * class_label\n",
    "        return nv,X,y\n",
    "\n",
    "    \"\"\" Generate the non-sensitive features randomly \"\"\"\n",
    "    # We will generate one gaussian cluster for each class\n",
    "    mu1, sigma1 = [2, 2], [[5, 1], [1, 5]]\n",
    "    mu2, sigma2 = [-2,-2], [[10, 1], [1, 3]]\n",
    "    nv1, X1, y1 = gen_gaussian(mu1, sigma1, 1) # positive class\n",
    "    nv2, X2, y2 = gen_gaussian(mu2, sigma2, -1) # negative class\n",
    "\n",
    "    # join the posisitve and negative class clusters\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.hstack((y1, y2))\n",
    "\n",
    "    # shuffle the data\n",
    "    perm = list(range(0,n_samples*2))\n",
    "    shuffle(perm)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    rotation_mult = np.array([[math.cos(disc_factor), -math.sin(disc_factor)], [math.sin(disc_factor), math.cos(disc_factor)]])\n",
    "    X_aux = np.dot(X, rotation_mult)\n",
    "\n",
    "\n",
    "    \"\"\" Generate the sensitive feature here \"\"\"\n",
    "    x_control = [] # this array holds the sensitive feature value\n",
    "    for i in range (0, len(X)):\n",
    "        x = X_aux[i]\n",
    "\n",
    "        # probability for each cluster that the point belongs to it\n",
    "        p1 = nv1.pdf(x)\n",
    "        p2 = nv2.pdf(x)\n",
    "        \n",
    "        # normalize the probabilities from 0 to 1\n",
    "        s = p1+p2\n",
    "        p1 = p1/s\n",
    "        p2 = p2/s\n",
    "        \n",
    "        r = np.random.uniform() # generate a random number from 0 to 1\n",
    "\n",
    "        if r < p1: # the first cluster is the positive class\n",
    "            x_control.append(1.0) # 1.0 means its male\n",
    "        else:\n",
    "            x_control.append(0.0) # 0.0 -> female\n",
    "\n",
    "    x_control = np.array(x_control)\n",
    "\n",
    "    \"\"\" Show the data \"\"\"\n",
    "    if plot_data:\n",
    "        num_to_draw = 200 # we will only draw a small number of points to avoid clutter\n",
    "        x_draw = X[:num_to_draw]\n",
    "        y_draw = y[:num_to_draw]\n",
    "        x_control_draw = x_control[:num_to_draw]\n",
    "\n",
    "        X_s_0 = x_draw[x_control_draw == 0.0]\n",
    "        X_s_1 = x_draw[x_control_draw == 1.0]\n",
    "        y_s_0 = y_draw[x_control_draw == 0.0]\n",
    "        y_s_1 = y_draw[x_control_draw == 1.0]\n",
    "        plt.scatter(X_s_0[y_s_0==1.0][:, 0], X_s_0[y_s_0==1.0][:, 1], color='green', marker='x', s=30, linewidth=1.5, label= \"Prot. +ve\")\n",
    "        plt.scatter(X_s_0[y_s_0==-1.0][:, 0], X_s_0[y_s_0==-1.0][:, 1], color='red', marker='x', s=30, linewidth=1.5, label = \"Prot. -ve\")\n",
    "        plt.scatter(X_s_1[y_s_1==1.0][:, 0], X_s_1[y_s_1==1.0][:, 1], color='green', marker='o', facecolors='none', s=30, label = \"Non-prot. +ve\")\n",
    "        plt.scatter(X_s_1[y_s_1==-1.0][:, 0], X_s_1[y_s_1==-1.0][:, 1], color='red', marker='o', facecolors='none', s=30, label = \"Non-prot. -ve\")\n",
    "\n",
    "        \n",
    "        plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off') # dont need the ticks to see the data distribution\n",
    "        plt.tick_params(axis='y', which='both', left='off', right='off', labelleft='off')\n",
    "        plt.legend(loc=2, fontsize=15)\n",
    "        plt.xlim((-15,10))\n",
    "        plt.ylim((-10,15))\n",
    "        #plt.savefig(\"img/data.png\")\n",
    "        plt.show()\n",
    "\n",
    "    #x_control = {\"s1\": x_control} # all the sensitive features are stored in a dictionary\n",
    "    return X,y,x_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the adult data        \n",
    "def load_adult_data(load_data_size=None, drop_sensitive=True):\n",
    "\n",
    "   # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    attrs = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country','target'] \n",
    "    \n",
    "    data = pd.read_csv(\"dataset/adult/adult.data\", names=attrs, skipinitialspace=True) \n",
    "\n",
    "    data = data.replace(to_replace ='?', value =np.NaN)\n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    target_mapping = {'<=50K': -1, '>50K': 1,'<=50K.': -1, '>50K.': 1}\n",
    "    data.replace({\"target\": target_mapping}, inplace=True)\n",
    "    data['native_country'] = data['native_country'].apply(lambda x: 'USA' if x in ['United-States'] else 'NON-USA')\n",
    "    data['education'] = data['education'].apply(lambda x: 'pre-middle-school' if x in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"] else 'high-school')\n",
    "\n",
    "    encoded_object_df = pd.DataFrame()\n",
    "\n",
    "    for column in ['workclass', 'sex', 'education', 'marital_status', 'occupation','relationship',  'native_country']:\n",
    "        # race\n",
    "        encoded_object_df = pd.concat([encoded_object_df,pd.get_dummies(data[column], prefix=column, drop_first=True)] ,axis=1)\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    cols_to_scale = ['age', 'education_num', 'capital_gain', 'capital_loss', 'capital_loss', 'target']\n",
    "    #fnlwgt\n",
    "\n",
    "    encoded_int_df = data[cols_to_scale]\n",
    "\n",
    "    encoded_int_df[cols_to_scale] = min_max_scaler.fit_transform(encoded_int_df[cols_to_scale])\n",
    "    \n",
    "    final_df = pd.concat([encoded_object_df, encoded_int_df], axis=1)\n",
    "    #final_df = final_df.sample(n=load_data_size)\n",
    "    final_df = final_df.iloc[0:load_data_size]\n",
    "    \n",
    "    # shuffle the data\n",
    "    \n",
    "    y = np.where(final_df['target'] == 0, -1, 1)\n",
    "    x_sensitive = np.array(final_df['sex_Male'])\n",
    "    \n",
    "    if drop_sensitive:\n",
    "        final_df.drop(['sex_Male'], axis=1, inplace=True)\n",
    "        \n",
    "    final_df.drop(['target'], axis=1, inplace=True)\n",
    "        \n",
    "    X = np.array(final_df)\n",
    "    \n",
    "    print(f\"Shapes of X = {X.shape}, y={y.shape}, x_sensitive={x_sensitive.shape}\")\n",
    "    \n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load bank data\n",
    "def load_bank_data(load_data_size=None, drop_sensitive=True):\n",
    "\n",
    "   # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    attrs = [\"age\",\"job\",\"marital\",\"education\",\"default\",\"balance\",\"housing\",\"loan\",\"contact\",\"day\",\"month\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"poutcome\",\"y\"] \n",
    "    \n",
    "    data = pd.read_csv(\"dataset/bank/bank.csv\", skipinitialspace=True, delimiter=';') \n",
    "\n",
    "    data['month-seasonal'] = data['month'].apply(lambda x: 'q1' if x in [\"jan\", \"feb\", \"mar\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q2' if x in [\"apr\", \"may\", \"jun\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q3' if x in [\"jul\", \"aug\", \"sep\"] else x )\n",
    "    data['month-seasonal'] = data['month-seasonal'].apply(lambda x: 'q4' if x in [\"oct\", \"nov\", \"dec\"] else x )\n",
    "    data['age-range'] = data['age'].apply(lambda x: 0.0 if (x >=25 and x<=60)  else 1.0 )\n",
    "\n",
    "    data.drop(['month'], inplace=True, axis=1)\n",
    "    \n",
    "    encoded_object_df = pd.DataFrame()\n",
    "\n",
    "    for column in ['job', 'marital', 'education', 'default', 'housing','loan','contact', 'poutcome','y','month-seasonal']:\n",
    "        encoded_object_df = pd.concat([encoded_object_df,pd.get_dummies(data[column], prefix=column, drop_first=True)] ,axis=1)\n",
    "    \n",
    "    min_max_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    cols_to_scale = ['balance','day','duration', 'campaign', 'pdays','previous']\n",
    "\n",
    "    encoded_int_df = data[cols_to_scale]\n",
    "\n",
    "    encoded_int_df[cols_to_scale] = min_max_scaler.fit_transform(encoded_int_df[cols_to_scale])\n",
    "    \n",
    "    final_df = pd.concat([encoded_object_df, encoded_int_df, data['age-range']], axis=1)\n",
    "    \n",
    "    final_df = final_df.sample(n=load_data_size, random_state=1234)\n",
    "    \n",
    "    y = np.where(final_df['y_yes'] == 0, -1, 1)\n",
    "    x_sensitive = np.array(final_df['age-range'])\n",
    "\n",
    "    \n",
    "    if drop_sensitive:\n",
    "        final_df.drop(['age-range'], axis=1, inplace=True)\n",
    "        \n",
    "    final_df.drop(['y_yes'], axis=1, inplace=True)\n",
    "        \n",
    "    X = np.array(final_df)\n",
    "    \n",
    "    print(f\"Shapes of X = {X.shape}, y={y.shape}, x_sensitive={x_sensitive.shape}\")\n",
    "    \n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cov_thresh_vs_acc_pos_ratio(x_all, y_all, x_control_all, num_folds, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, sensitive_attrs):\n",
    "\n",
    "\n",
    "    # very the covariance threshold using a range of decreasing multiplicative factors and see the tradeoffs between accuracy and fairness\n",
    "    it = 0.05\n",
    "    cov_range = np.arange(1.0, 0.0-it, -it).tolist()\n",
    "    if apply_accuracy_constraint == True:\n",
    "        if sep_constraint == False:\n",
    "            it = 0.1\n",
    "            cov_range = np.arange(0.0, 1.0 + it, it).tolist()\n",
    "        if sep_constraint == True:\n",
    "            cov_range =  [0,1,5,10,20,50,100,500,1000]\n",
    "\n",
    "    \n",
    "    positive_class_label = 1 # positive class is +1\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    positive_per_category = defaultdict(list) # for each category (male / female), the frac of positive\n",
    "\n",
    "    # first get the original values of covariance in the unconstrained classifier -- these original values are not needed for reverse constraint    \n",
    "    test_acc_arr, train_acc_arr, correlation_dict_test_arr, correlation_dict_train_arr, cov_dict_test_arr, cov_dict_train_arr = compute_cross_validation_error(x_all, y_all, x_control_all, num_folds, loss_function, 0, apply_accuracy_constraint, sep_constraint, sensitive_attrs, [{} for i in range(0,num_folds)], 0)\n",
    "\n",
    "    for c in cov_range:\n",
    "        print((\"LOG: testing for multiplicative factor: %0.2f\" % c)) \n",
    "        sensitive_attrs_to_cov_original_arr_multiplied = []\n",
    "        for sensitive_attrs_to_cov_original in cov_dict_train_arr:\n",
    "            sensitive_attrs_to_cov_thresh = deepcopy(sensitive_attrs_to_cov_original)\n",
    "            for k in list(sensitive_attrs_to_cov_thresh.keys()):\n",
    "                v = sensitive_attrs_to_cov_thresh[k]\n",
    "                if type(v) == type({}):\n",
    "                    for k1 in list(v.keys()):\n",
    "                        v[k1] = v[k1] * c\n",
    "                else:\n",
    "                    sensitive_attrs_to_cov_thresh[k] = v * c\n",
    "            sensitive_attrs_to_cov_original_arr_multiplied.append(sensitive_attrs_to_cov_thresh)\n",
    "\n",
    "\n",
    "        test_acc_arr, train_acc_arr, correlation_dict_test_arr, correlation_dict_train_arr, cov_dict_test_arr, cov_dict_train_arr  = compute_cross_validation_error(x_all, y_all, x_control_all, num_folds, loss_function, apply_fairness_constraints, apply_accuracy_constraint, sep_constraint, sensitive_attrs, sensitive_attrs_to_cov_original_arr_multiplied, c)\n",
    "        test_acc.append(np.mean(test_acc_arr))\n",
    "\n",
    "\n",
    "        correlation_dict_train = get_avg_correlation_dict(correlation_dict_train_arr)\n",
    "        correlation_dict_test = get_avg_correlation_dict(correlation_dict_test_arr)\n",
    "        \n",
    "        # just plot the correlations for the first sensitive attr, the plotting can be extended for the other values, but as a proof of concept, we will jsut show for one\n",
    "        s = sensitive_attrs[0]    \n",
    "        \n",
    "        for k,v in list(correlation_dict_test[s].items()):\n",
    "            if v.get(positive_class_label) is None:\n",
    "                positive_per_category[k].append(0.0)\n",
    "            else:\n",
    "                positive_per_category[k].append(v[positive_class_label])\n",
    "    \n",
    "    positive_per_category = dict(positive_per_category)\n",
    "    \n",
    "    p_rule_arr = (np.array(positive_per_category[0]) / np.array(positive_per_category[1])) * 100.0\n",
    "    \n",
    "\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    plt.plot(cov_range, positive_per_category[0], \"-o\" , color=\"green\", label = \"Protected\")\n",
    "    plt.plot(cov_range, positive_per_category[1], \"-o\", color=\"blue\", label = \"Non-protected\")\n",
    "    ax.set_xlim([min(cov_range), max(cov_range)])\n",
    "    plt.xlabel('Multiplicative loss factor')\n",
    "    plt.ylabel('Perc. in positive class')\n",
    "    if apply_accuracy_constraint == False:\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.xlabel('Multiplicative covariance factor (c)')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = plt.subplot(2,1,2)\n",
    "    plt.scatter(p_rule_arr, test_acc, color=\"red\")\n",
    "    ax.set_xlim([min(p_rule_arr), max(max(p_rule_arr), 100)])\n",
    "    plt.xlabel('P% rule')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def get_line_coordinates(w, x1, x2):\n",
    "    y1 = (-w[0] - (w[1] * x1)) / w[2]\n",
    "    y2 = (-w[0] - (w[1] * x2)) / w[2]    \n",
    "    return y1,y2\n",
    "\n",
    "    def plot_boundaries(w1, w2, p1, p2, acc1, acc2, fname):\n",
    "\n",
    "        num_to_draw = 200 # we will only draw a small number of points to avoid clutter\n",
    "        x_draw = X[:num_to_draw]\n",
    "        y_draw = y[:num_to_draw]\n",
    "        x_control_draw = x_control[\"s1\"][:num_to_draw]\n",
    "\n",
    "        X_s_0 = x_draw[x_control_draw == 0.0]\n",
    "        X_s_1 = x_draw[x_control_draw == 1.0]\n",
    "        y_s_0 = y_draw[x_control_draw == 0.0]\n",
    "        y_s_1 = y_draw[x_control_draw == 1.0]\n",
    "        plt.scatter(X_s_0[y_s_0==1.0][:, 1], X_s_0[y_s_0==1.0][:, 2], color='green', marker='x', s=30, linewidth=1.5)\n",
    "        plt.scatter(X_s_0[y_s_0==-1.0][:, 1], X_s_0[y_s_0==-1.0][:, 2], color='red', marker='x', s=30, linewidth=1.5)\n",
    "        plt.scatter(X_s_1[y_s_1==1.0][:, 1], X_s_1[y_s_1==1.0][:, 2], color='green', marker='o', facecolors='none', s=30)\n",
    "        plt.scatter(X_s_1[y_s_1==-1.0][:, 1], X_s_1[y_s_1==-1.0][:, 2], color='red', marker='o', facecolors='none', s=30)\n",
    "\n",
    "\n",
    "        x1,x2 = max(x_draw[:,1]), min(x_draw[:,1])\n",
    "        y1,y2 = ut.get_line_coordinates(w1, x1, x2)\n",
    "        plt.plot([x1,x2], [y1,y2], 'c-', linewidth=3, label = \"Acc=%0.2f; p%% rule=%0.0f%% - Original\"%(acc1, p1))\n",
    "        y1,y2 = ut.get_line_coordinates(w2, x1, x2)\n",
    "        plt.plot([x1,x2], [y1,y2], 'b--', linewidth=3, label = \"Acc=%0.2f; p%% rule=%0.0f%% - Constrained\"%(acc2, p2))\n",
    "\n",
    "\n",
    "\n",
    "        plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off') # dont need the ticks to see the data distribution\n",
    "        plt.tick_params(axis='y', which='both', left='off', right='off', labelleft='off')\n",
    "        plt.legend(loc=2, fontsize=15)\n",
    "        plt.xlim((-15,10))\n",
    "        plt.ylim((-10,15))\n",
    "        plt.savefig(fname)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(C_values, sample_count, data_type, correlation=0.0):\n",
    "    if data_type == 'bank':\n",
    "        X, y, x_control = load_bank_data(load_data_size=sample_count)\n",
    "    elif (data_type == 'synthetic'):\n",
    "        X, y, x_control = load_synthetic_data(load_data_size=sample_count, plot_data=True)\n",
    "    else:\n",
    "        X, y, x_control = load_adult_data(load_data_size=sample_count)\n",
    "        \n",
    "    ## compute the p_rule on base scenario\n",
    "    compute_p_rule(\"Base Data \" + data_type, x_control, y)\n",
    "    \n",
    "    for C in C_values:\n",
    "        for feature in [None, x_control]:\n",
    "            p_feature = None\n",
    "            if (feature is not None):\n",
    "                p_feature = 'With Fairness Constraints'\n",
    "            print(f'C={C},{p_feature}')\n",
    "            clf = SupportVectorMachine(kernel=rbf_kernel, sensible_feature=feature, C=C, power=4, coef=1, correlation=correlation)\n",
    "            #clf = SupportVectorMachine_trial(kernel=rbf_kernel, sensible_feature=feature, C=C, power=4, coef=1)\n",
    "            clf.fit(X, y)\n",
    "            y_pred = clf.predict(X)\n",
    "\n",
    "            accuracy = accuracy_score(y, y_pred)\n",
    "            print (\"Accuracy:\", accuracy)\n",
    "            p_rule_string = \"p_rule on = \" + str(p_feature) + \" , \" + data_type\n",
    "            compute_p_rule(p_rule_string, x_control, y_pred)   \n",
    "            \n",
    "            X_svm, y_svm, x_control_svm = X, y, x_control\n",
    "            svc = svm.SVC(kernel='rbf', C=1).fit(X_svm, y_svm)\n",
    "\n",
    "            y_pred_svm = svc.predict(X_svm)\n",
    "            accuracy_svm = accuracy_score(y_svm, y_pred_svm)\n",
    "            print (\"Accuracy of SVM Classifier:\", accuracy_svm)\n",
    "            \n",
    "            compute_p_rule(\"p_rule on svm classifier on \" + data_type, x_control, y_pred_svm)\n",
    "            print('**'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_control= [0. 1. 1. ... 0. 0. 0.]\n",
      "class_labels= [-1.  1.  1. ... -1. -1.  1.]\n",
      "\n",
      "Total data points: 4000\n",
      "# non-protected examples: 1802\n",
      "# protected examples: 2198\n",
      "Non-protected in positive class: 1247 (69%)\n",
      "Protected in positive class: 753 (34%)\n",
      "P-rule is: 50%\n",
      "Base Data synthetic = 49.50574736966536\n",
      "C=1,None\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3642e+03 -9.3181e+03  5e+04  3e+00  7e-15\n",
      " 1: -8.7492e+02 -5.6021e+03  6e+03  1e-01  6e-15\n",
      " 2: -9.2016e+02 -1.6502e+03  8e+02  2e-02  5e-15\n",
      " 3: -9.8737e+02 -1.1536e+03  2e+02  3e-03  5e-15\n",
      " 4: -1.0028e+03 -1.0765e+03  7e+01  8e-04  5e-15\n",
      " 5: -1.0093e+03 -1.0450e+03  4e+01  3e-04  5e-15\n",
      " 6: -1.0126e+03 -1.0302e+03  2e+01  1e-04  5e-15\n",
      " 7: -1.0146e+03 -1.0219e+03  7e+00  3e-05  6e-15\n",
      " 8: -1.0151e+03 -1.0201e+03  5e+00  2e-05  6e-15\n",
      " 9: -1.0157e+03 -1.0181e+03  2e+00  5e-06  6e-15\n",
      "10: -1.0160e+03 -1.0171e+03  1e+00  2e-06  6e-15\n",
      "11: -1.0162e+03 -1.0167e+03  6e-01  8e-07  6e-15\n",
      "12: -1.0163e+03 -1.0165e+03  2e-01  3e-08  7e-15\n",
      "13: -1.0163e+03 -1.0164e+03  1e-01  1e-08  6e-15\n",
      "14: -1.0163e+03 -1.0164e+03  4e-02  2e-09  6e-15\n",
      "15: -1.0163e+03 -1.0164e+03  3e-02  1e-09  6e-15\n",
      "16: -1.0163e+03 -1.0164e+03  1e-02  5e-10  6e-15\n",
      "17: -1.0163e+03 -1.0163e+03  8e-03  1e-10  6e-15\n",
      "18: -1.0163e+03 -1.0163e+03  2e-03  2e-11  7e-15\n",
      "19: -1.0163e+03 -1.0163e+03  7e-04  2e-12  7e-15\n",
      "Optimal solution found.\n",
      "Accuracy: 0.88175\n",
      "x_control= [0. 1. 1. ... 0. 0. 0.]\n",
      "class_labels= [-1.  1.  1. ... -1. -1.  1.]\n",
      "\n",
      "Total data points: 4000\n",
      "# non-protected examples: 1802\n",
      "# protected examples: 2198\n",
      "Non-protected in positive class: 1269 (70%)\n",
      "Protected in positive class: 646 (29%)\n",
      "P-rule is: 42%\n",
      "p_rule on = None , synthetic = 41.734767117610325\n",
      "Accuracy of SVM Classifier: 0.8805\n",
      "x_control= [0. 1. 1. ... 0. 0. 0.]\n",
      "class_labels= [-1.  1.  1. ... -1. -1.  1.]\n",
      "\n",
      "Total data points: 4000\n",
      "# non-protected examples: 1802\n",
      "# protected examples: 2198\n",
      "Non-protected in positive class: 1273 (71%)\n",
      "Protected in positive class: 661 (30%)\n",
      "P-rule is: 43%\n",
      "p_rule on svm classifier on synthetic = 42.56965734042302\n",
      "****************************************************************************************************\n",
      "C=1,With Fairness Constraints\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3642e+03 -9.3298e+03  5e+04  3e+00  6e-15\n",
      " 1: -8.7797e+02 -5.6127e+03  6e+03  1e-01  7e-15\n",
      " 2: -9.1972e+02 -1.6514e+03  8e+02  2e-02  5e-15\n",
      " 3: -9.8583e+02 -1.1552e+03  2e+02  3e-03  5e-15\n",
      " 4: -1.0016e+03 -1.0748e+03  7e+01  8e-04  5e-15\n",
      " 5: -1.0076e+03 -1.0461e+03  4e+01  4e-04  5e-15\n",
      " 6: -1.0114e+03 -1.0291e+03  2e+01  1e-04  6e-15\n",
      " 7: -1.0135e+03 -1.0211e+03  8e+00  3e-05  6e-15\n",
      " 8: -1.0142e+03 -1.0182e+03  4e+00  1e-05  6e-15\n",
      " 9: -1.0146e+03 -1.0170e+03  2e+00  2e-06  6e-15\n",
      "10: -1.0150e+03 -1.0159e+03  1e+00  6e-07  6e-15\n",
      "11: -1.0151e+03 -1.0155e+03  4e-01  1e-07  6e-15\n",
      "12: -1.0152e+03 -1.0154e+03  3e-01  6e-08  6e-15\n",
      "13: -1.0152e+03 -1.0153e+03  1e-01  4e-09  7e-15\n",
      "14: -1.0152e+03 -1.0153e+03  6e-02  2e-09  6e-15\n",
      "15: -1.0152e+03 -1.0153e+03  3e-02  6e-10  6e-15\n",
      "16: -1.0152e+03 -1.0153e+03  1e-02  2e-10  6e-15\n",
      "17: -1.0152e+03 -1.0152e+03  6e-03  4e-11  6e-15\n",
      "18: -1.0152e+03 -1.0152e+03  2e-03  8e-12  6e-15\n",
      "19: -1.0152e+03 -1.0152e+03  4e-04  2e-12  6e-15\n",
      "Optimal solution found.\n",
      "Accuracy: 0.8785\n",
      "x_control= [0. 1. 1. ... 0. 0. 0.]\n",
      "class_labels= [-1.  1.  1. ... -1. -1.  1.]\n",
      "\n",
      "Total data points: 4000\n",
      "# non-protected examples: 1802\n",
      "# protected examples: 2198\n",
      "Non-protected in positive class: 1266 (70%)\n",
      "Protected in positive class: 678 (31%)\n",
      "P-rule is: 44%\n",
      "p_rule on = With Fairness Constraints , synthetic = 43.90592050506923\n",
      "Accuracy of SVM Classifier: 0.8805\n",
      "x_control= [0. 1. 1. ... 0. 0. 0.]\n",
      "class_labels= [-1.  1.  1. ... -1. -1.  1.]\n",
      "\n",
      "Total data points: 4000\n",
      "# non-protected examples: 1802\n",
      "# protected examples: 2198\n",
      "Non-protected in positive class: 1273 (71%)\n",
      "Protected in positive class: 661 (30%)\n",
      "P-rule is: 43%\n",
      "p_rule on svm classifier on synthetic = 42.56965734042302\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "data_count=2000\n",
    "run_experiment([1], sample_count=data_count, data_type='synthetic')\n",
    "#run_experiment([10], sample_count=data_count, data_type='bank')\n",
    "#run_experiment([1], sample_count=data_count, data_type='adult', correlation=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
